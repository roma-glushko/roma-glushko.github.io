{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/k-nearest-neighbors/","result":{"data":{"markdownRemark":{"html":"<p>K-Nearest Neighbors (a.k.a KNN) is one of the famous machine learning approaches. It's simple, intuitive and useful tool to have in your machine learning toolbox.</p>\n<p>KNN is instance-based learning algorithm. It doesn't make any assumption about dataset relations and uses provided samples in order to estimate new items.</p>\n<h2 id=\"intuition\" style=\"position:relative;\"><a href=\"#intuition\" aria-label=\"intuition permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Intuition</h2>\n<p>As old saying goes, you are who you surround yourself with. This idea even has mathematical interpretation. In N-dimensional space, points that are closer to each other are more similar, then ones that stand apart. This is the assumption that KNN algorithm makes. It tries to find K nearest known points or samples, also called \"neighbors\", to some unknown sample. Majority class of neighbors defines class of the unknown sample.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dbb3359885e09bbbb66596868e221934/8ae3e/k-nearest-neighbors.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 74.11764705882352%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACRUlEQVQ4y31Tz08TQRj9OrsgXaAV0ZY1VNNgf+yCkY3giWhiohGJXkEMAWOMSjwINGpJ2S3+iDGaqNEYD548eNaz/gccPHHw4FXReNBEI5qw45vtTNltipN8eW9+vXnffDNEsnHPriP3rBovdhIXSBSjGpYQhySf84lyggMZ+owamxKNHBCax8YC+jvl+AhiL0Xn/998Q1ML2xGDanxDOsZYBaIHgC3AEzzqzmISR3zPuh/wY+lA0WeBs+tq7Y/NK9gGMR3RAz6lhBTGJHZBcEDw91dzsfChp4jaHsOtFMvDqR0xVSeutWXaP8vFybfjvcHGj4yGvxPdFfw30TkInJFCLJyqIXEPChHck+9a/XzZLgj+aT63uL5YDKq5vs80RFH211LWlatfRBpwDNElhN4g2hGjiLIUvAzx85F0jKjzJaLWIaKOUIUvQTAtBDO4M70+4Vqa4kezRoLfHOjmE+jcfqDp7pJzZGZGOauuEDlfiXaAzza9L+7asjD28IZrOc/HTNP37JfcTSWo+irOPO9R3KvUiqJpu6eJkmtIE4LH4XAX8I56xEk4nYfTuOxPwunZgN/q716ds9kWNYuHUtYhmFWF2Y64wUOpi3bQbBMbtEgWz4aCfgpiHTFqFfxzQ5Wp+QO37yFOqvG/Fevht2sF50/FWvhSyl8M1rw7zJJs89upPx+IIjSBvjwAKWcgmOBPB1uejPZ0rlzIOi9Om+nX45neD1f6UmLN2kK+SSHKhToq7oceOniJV+3lxn2rs31NL/UfK03ecKA4ho0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Nearest neighbors defines class of unknown sample\"\n        title=\"Nearest neighbors defines class of unknown sample\"\n        src=\"/static/dbb3359885e09bbbb66596868e221934/c5bb3/k-nearest-neighbors.png\"\n        srcset=\"/static/dbb3359885e09bbbb66596868e221934/04472/k-nearest-neighbors.png 170w,\n/static/dbb3359885e09bbbb66596868e221934/9f933/k-nearest-neighbors.png 340w,\n/static/dbb3359885e09bbbb66596868e221934/c5bb3/k-nearest-neighbors.png 680w,\n/static/dbb3359885e09bbbb66596868e221934/8ae3e/k-nearest-neighbors.png 756w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<div class=\"image-title\">Nearest neighbors defines class of unknown sample</div>\n<p>To came from this idea to implementation, we need to define how to find closest points.</p>\n<h2 id=\"closest-points\" style=\"position:relative;\"><a href=\"#closest-points\" aria-label=\"closest points permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Closest Points</h2>\n<p>We need to somehow measure distance between samples in order to find closest (hence similar) neighbors. Turned out there are quite a few metrics we can use:</p>\n<ul>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Euclidean Distance</a> - straight-line distance between two points</li>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Taxicab_geometry\">Manhattan (City Block) Distance</a> - distance between two points traversed along one axis at the time at right angle</li>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Minkowski_distance\">Minkowski Distance</a> - Generalization of Euclidean and Manhattan distances</li>\n<li>scikit-learn supports <a target=\"_blank\" rel=\"noopener\" href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric\">other metrics</a> as well</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f8f6d672a5fb107dd1d6040218c5088d/966c1/distance-metrics.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 36.47058823529412%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAABN0lEQVQoz01Ry07DMBCc9SNtkzohkBKakKIiEEI9wAEkxKGXiAsgoP0APoAzn8IH8iewaztqpWy8nt0Zz9oAoDQHfxyA1ZJoyQ3jWuqfhTZbl8lWRUwnJByQcHIrq+crBDHgbgzz5tp6aFJEdusuJMU67Y7X6TT3OBKyREby1UhPNq6tEDUIpL00kClLKBd6k8+n/cnzpXdsBJci8MLxnt7WrVuW3j37sCYl5Nf2K+vyfvbQipTViOoJOzqUTb88ax67aiQk4sl4LY+SVxb/HT2d3zT3DWyoQXqmwvm4WtSrmTPhoGgXYoeo5HUMzyAbYC1NhaJA5sFIToocxxw3cFTE/c8EWX83p/nEv8/uMGI3B6E5kEwU9I/39/MNl+y0KBZk5Bb+zrDvXNTnHA3CmLTHqSKnGPB/YCwRrPTRHkoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Different distance metrics and their boundaries\"\n        title=\"Different distance metrics and their boundaries\"\n        src=\"/static/f8f6d672a5fb107dd1d6040218c5088d/c5bb3/distance-metrics.png\"\n        srcset=\"/static/f8f6d672a5fb107dd1d6040218c5088d/04472/distance-metrics.png 170w,\n/static/f8f6d672a5fb107dd1d6040218c5088d/9f933/distance-metrics.png 340w,\n/static/f8f6d672a5fb107dd1d6040218c5088d/c5bb3/distance-metrics.png 680w,\n/static/f8f6d672a5fb107dd1d6040218c5088d/b12f7/distance-metrics.png 1020w,\n/static/f8f6d672a5fb107dd1d6040218c5088d/b5a09/distance-metrics.png 1360w,\n/static/f8f6d672a5fb107dd1d6040218c5088d/966c1/distance-metrics.png 1694w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<div class=\"image-title\">Distance metrics and their boundaries in 2D</div>\n<p>Euclidean and Manhattan distances are two the most popular distance metrics:</p>\n$$\nD(x, y)_{euclidean} = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}\n$$\n$$\nD(x, y)_{manhattan} = \\sum_{i=1}^n |x_i-y_i|\n$$\n<p>where $x=(x_1, x_2, ..., x_n)$ and $y=(y_1, y_2, ..., y_n)$ are two points in the N-dimensional space.</p>\n<p>Minkowski distance incorporates both Euclidean (when p is 2) and Manhattan (when p is 1) distances and gives additional degree of flexibility via p param:</p>\n$$\nD(x, y)_{minkowski} = \\left(\\sum_{i=1}^n (|x_i-y_i|)^p\\right)^\\frac{1}{p}\n$$\n<p>These distance metrics are intuitive and fast to compute.</p>\n<p>However, all features or dimensions are treated the same way. This may be an issue if features have values in different scales. For example,</p>\n<ul>\n<li>customer age value is normally between 18 and 100</li>\n<li>annual income values lies between 20,000 and 200,000 (and sometimes goes far beyond)</li>\n</ul>\n<p>When calculating distance metrics, this leads to a situation when major part of similarity actually comes from large scale-value features (like annual income). As a result, the rest of the features will be almost neglected and won't be really taken into account during similarity checking.</p>\n<h2 id=\"standardization\" style=\"position:relative;\"><a href=\"#standardization\" aria-label=\"standardization permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Standardization</h2>\n<p>Ignoring small-scale features in the similarity checking is rarely a desired effect. We would like rather to avoid it to get real neighbors of our sample.</p>\n<p>We can try to rescale all features to the same range (0-1, for example). This will standardize or normalize features and let them have the same influence in the distance calculations.</p>\n$$\nZ = \\frac{x - \\overline{x}}{S}\n$$\n<p>where $x$ is sample feature value, $\\overline{x}$ is a feature mean and $S$ is a feature standard deviation.</p>\n<p>Z is a <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Standard_score\">standard score (or z-score)</a> of the sample feature value. It's a dimensionless quantity and it shows how \"unusual\" or far sample feature value from the feature mean (which is usual or expected value).</p>\n<p>Though normalization changes values of features, it doesn't change feature distribution shape.</p>\n<h2 id=\"dense-dataset\" style=\"position:relative;\"><a href=\"#dense-dataset\" aria-label=\"dense dataset permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Dense Dataset</h2>\n<p>We assume that similar samples are close in the N-dimensional feature space. Is this always a case?</p>\n<p>Turned out that with growing number of dimensions (or features), number of samples should grow exponentially in order to keep the same density of sample points in the space. High-dimensional space contains far more actual space between points than our intuition may suggest.</p>\n<p>This may lead to a situation when distance to neighbors across all feature axis are just slightly bigger than average distance between points. In sparse high-dimensional feature space we are loosing our notion of nearest points.</p>\n<p>This is the reason why ratio of features to samples is important for KNN algorithm.</p>\n<h2 id=\"neighborhood-size\" style=\"position:relative;\"><a href=\"#neighborhood-size\" aria-label=\"neighborhood size permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Neighborhood Size</h2>\n<p>Effectiveness of applying KNN depends greatly on choosing appropriate size of neighborhood a.k.a <strong>K</strong> hyperparam. In turn this choice is determined by dataset. It's usual to see K in range from 1 to 20 (5 is default size in scikit-learn).</p>\n<p>Smaller value of K adds more sensitivity and variance. For noisy datasets this means that given a slightly adjusted dataset we may get a different neighbors.  </p>\n<p>Increasing K hyperparam adds more bias to KNN results and smoother decision boundaries. If K param value is too big, KNN will loose its ability to find local structures in the dataset.</p>\n<p>In general, neighborhood size is subject of hyperparam tuning. Final decision should be validated on separated validation dataset.</p>\n<h2 id=\"applications\" style=\"position:relative;\"><a href=\"#applications\" aria-label=\"applications permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Applications</h2>\n<p>You may find KNN-based classificators and regressors which is a trivial application of KNN idea. Let's review a few less obvious applications where KNN shines even better.</p>\n<h3 id=\"feature-engineering\" style=\"position:relative;\"><a href=\"#feature-engineering\" aria-label=\"feature engineering permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Feature Engineering</h3>\n<p>KNN is able to identify local structures in the dataset. This information is useful clue itself and local structure labels may be used in classification/regression pipeline as an additional feature.</p>\n<h3 id=\"neighbors-in-embeddings\" style=\"position:relative;\"><a href=\"#neighbors-in-embeddings\" aria-label=\"neighbors in embeddings permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Neighbors in Embeddings</h3>\n<p>KNN algorithm becomes useful when you have embeddings - samples projected into lower dimensional latent space. In embedding space distance means similarity and KNN can suggest similar items. This is especially useful for recommendation systems. In that case, nearest neighbor search can be optimized further by leveraging <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Locality-sensitive_hashing\">locality-sensitive hashing</a>, for instance.</p>\n<h3 id=\"anomaly-detection\" style=\"position:relative;\"><a href=\"#anomaly-detection\" aria-label=\"anomaly detection permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Anomaly Detection</h3>\n<p>Distances to K nearest neighbors can be viewed as an estimation of local density. If distance is relatively large, this implies that density is low and sample is likely to be an outlier.</p>\n<h2 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Summary</h2>\n<p>I hope we shed a bit of light on theoretical aspects of K-Nearest Neighbor algorithm and proved that this pure idea is still useful after decades of being formulated.</p>\n<h2 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>References</h2>\n<ul>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/\">Practical Statistics for Data Scientists by Peter Bruce and Andrew Bruce</a></li>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d\">k-Nearest Neighbors and the Curse of Dimensionality</a></li>\n</ul>","timeToRead":5,"rawMarkdownBody":"\nK-Nearest Neighbors (a.k.a KNN) is one of the famous machine learning approaches. It's simple, intuitive and useful tool to have in your machine learning toolbox.\n\nKNN is instance-based learning algorithm. It doesn't make any assumption about dataset relations and uses provided samples in order to estimate new items.\n\n## Intuition\n\nAs old saying goes, you are who you surround yourself with. This idea even has mathematical interpretation. In N-dimensional space, points that are closer to each other are more similar, then ones that stand apart. This is the assumption that KNN algorithm makes. It tries to find K nearest known points or samples, also called \"neighbors\", to some unknown sample. Majority class of neighbors defines class of the unknown sample.\n\n![Nearest neighbors defines class of unknown sample](./img/k-nearest-neighbors.png \"Nearest neighbors defines class of unknown sample\")\n<div class=\"image-title\">Nearest neighbors defines class of unknown sample</div>\n\nTo came from this idea to implementation, we need to define how to find closest points.\n\n## Closest Points\n\nWe need to somehow measure distance between samples in order to find closest (hence similar) neighbors. Turned out there are quite a few metrics we can use:\n\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Euclidean Distance</a> - straight-line distance between two points\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Taxicab_geometry\">Manhattan (City Block) Distance</a> - distance between two points traversed along one axis at the time at right angle\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Minkowski_distance\">Minkowski Distance</a> - Generalization of Euclidean and Manhattan distances\n- scikit-learn supports <a target=\"_blank\" rel=\"noopener\" href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric\">other metrics</a> as well\n\n![Different distance metrics and their boundaries](./img/distance-metrics.png?22 \"Different distance metrics and their boundaries\")\n<div class=\"image-title\">Distance metrics and their boundaries in 2D</div>\n\nEuclidean and Manhattan distances are two the most popular distance metrics:\n\n$$\nD(x, y)_{euclidean} = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}\n$$\n\n$$\nD(x, y)_{manhattan} = \\sum_{i=1}^n |x_i-y_i|\n$$\n\nwhere $x=(x_1, x_2, ..., x_n)$ and $y=(y_1, y_2, ..., y_n)$ are two points in the N-dimensional space.\n\nMinkowski distance incorporates both Euclidean (when p is 2) and Manhattan (when p is 1) distances and gives additional degree of flexibility via p param:\n\n$$\nD(x, y)_{minkowski} = \\left(\\sum_{i=1}^n (|x_i-y_i|)^p\\right)^\\frac{1}{p}\n$$\n\nThese distance metrics are intuitive and fast to compute.\n\nHowever, all features or dimensions are treated the same way. This may be an issue if features have values in different scales. For example,\n\n- customer age value is normally between 18 and 100\n- annual income values lies between 20,000 and 200,000 (and sometimes goes far beyond)\n\nWhen calculating distance metrics, this leads to a situation when major part of similarity actually comes from large scale-value features (like annual income). As a result, the rest of the features will be almost neglected and won't be really taken into account during similarity checking.\n\n## Standardization\n\nIgnoring small-scale features in the similarity checking is rarely a desired effect. We would like rather to avoid it to get real neighbors of our sample.\n\nWe can try to rescale all features to the same range (0-1, for example). This will standardize or normalize features and let them have the same influence in the distance calculations.\n\n$$\nZ = \\frac{x - \\overline{x}}{S}\n$$\nwhere $x$ is sample feature value, $\\overline{x}$ is a feature mean and $S$ is a feature standard deviation.\n\nZ is a <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Standard_score\">standard score (or z-score)</a> of the sample feature value. It's a dimensionless quantity and it shows how \"unusual\" or far sample feature value from the feature mean (which is usual or expected value).\n\nThough normalization changes values of features, it doesn't change feature distribution shape.\n\n## Dense Dataset\n\nWe assume that similar samples are close in the N-dimensional feature space. Is this always a case?\n\nTurned out that with growing number of dimensions (or features), number of samples should grow exponentially in order to keep the same density of sample points in the space. High-dimensional space contains far more actual space between points than our intuition may suggest.\n\nThis may lead to a situation when distance to neighbors across all feature axis are just slightly bigger than average distance between points. In sparse high-dimensional feature space we are loosing our notion of nearest points.\n\nThis is the reason why ratio of features to samples is important for KNN algorithm.\n\n## Neighborhood Size\n\nEffectiveness of applying KNN depends greatly on choosing appropriate size of neighborhood a.k.a **K** hyperparam. In turn this choice is determined by dataset. It's usual to see K in range from 1 to 20 (5 is default size in scikit-learn).\n\nSmaller value of K adds more sensitivity and variance. For noisy datasets this means that given a slightly adjusted dataset we may get a different neighbors.  \n\nIncreasing K hyperparam adds more bias to KNN results and smoother decision boundaries. If K param value is too big, KNN will loose its ability to find local structures in the dataset.\n\nIn general, neighborhood size is subject of hyperparam tuning. Final decision should be validated on separated validation dataset.\n\n## Applications\n\nYou may find KNN-based classificators and regressors which is a trivial application of KNN idea. Let's review a few less obvious applications where KNN shines even better.\n\n### Feature Engineering\n\nKNN is able to identify local structures in the dataset. This information is useful clue itself and local structure labels may be used in classification/regression pipeline as an additional feature.\n### Neighbors in Embeddings\n\nKNN algorithm becomes useful when you have embeddings - samples projected into lower dimensional latent space. In embedding space distance means similarity and KNN can suggest similar items. This is especially useful for recommendation systems. In that case, nearest neighbor search can be optimized further by leveraging <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Locality-sensitive_hashing\">locality-sensitive hashing</a>, for instance.\n\n### Anomaly Detection\n\nDistances to K nearest neighbors can be viewed as an estimation of local density. If distance is relatively large, this implies that density is low and sample is likely to be an outlier.\n\n## Summary\n\nI hope we shed a bit of light on theoretical aspects of K-Nearest Neighbor algorithm and proved that this pure idea is still useful after decades of being formulated.\n\n## References\n\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/\">Practical Statistics for Data Scientists by Peter Bruce and Andrew Bruce</a>\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d\">k-Nearest Neighbors and the Curse of Dimensionality</a>","wordCount":{"words":963},"frontmatter":{"path":"/blog/k-nearest-neighbors/","humanDate":"Apr 5, 2021","fullDate":"2021-04-05","title":"K-Nearest Neighbors 👨‍👩‍👧‍👦","keywords":["machine learning"],"excerpt":"Theoretical aspects of the KNN algorithm. Where it can be applied and when it fails.","cover":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEDBP/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHPSDlRMP/EABkQAAMBAQEAAAAAAAAAAAAAAAABAhESMv/aAAgBAQABBQJVhVabLFI/PKP/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAaEAACAgMAAAAAAAAAAAAAAAAAARARMTJh/9oACAEBAAY/Asi4bNFluP/EABoQAAMBAQEBAAAAAAAAAAAAAAABESFRMUH/2gAIAQEAAT8hriSE2JGLrIFWuSltSCqvT4Ji2n//2gAMAwEAAgADAAAAEJPP/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8QR//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAECAQE/EFf/xAAeEAEAAgICAwEAAAAAAAAAAAABABExQSFhUZGh0f/aAAgBAQABPxBCYeDInZqMsro8xv1B4AKafq4AURvXXMuBSw5x+y4K3W5//9k=","aspectRatio":1.3322884012539185,"src":"/static/dab856139d3de9754d8bc45de82e7772/35aef/pexels-ollie-craig-6398503-min.jpg","srcSet":"/static/dab856139d3de9754d8bc45de82e7772/809fc/pexels-ollie-craig-6398503-min.jpg 850w,\n/static/dab856139d3de9754d8bc45de82e7772/4f4f6/pexels-ollie-craig-6398503-min.jpg 1700w,\n/static/dab856139d3de9754d8bc45de82e7772/35aef/pexels-ollie-craig-6398503-min.jpg 3400w,\n/static/dab856139d3de9754d8bc45de82e7772/9a201/pexels-ollie-craig-6398503-min.jpg 5100w,\n/static/dab856139d3de9754d8bc45de82e7772/4c51e/pexels-ollie-craig-6398503-min.jpg 6800w,\n/static/dab856139d3de9754d8bc45de82e7772/6b46a/pexels-ollie-craig-6398503-min.jpg 8000w","sizes":"(max-width: 3400px) 100vw, 3400px"}}}},"parent":{"__typename":"File","fields":{"gitLogLatestDate":"2021-05-17 23:30:30 +0300"}}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/how-i-built-my-ml-workstation/","title":"How I built my ML workstation 🔬"}}}}}