{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/nature-of-distributed-systems/","result":{"data":{"markdownRemark":{"html":"<p>The Internet evolves rapidly. We started  our own hardware servers, moved to managed hostings and now we have entered the era of cloud computing. The ever growing need to handle more and more traffic pushes us beyond the limits.</p>\n<p>As a result, it's so natural nowadays to spin off a few instances on AWS or GCP and start building one's own SaaS service that will scale in a way we need. Perhaps, your infrastructure already has load balancing, a couple of nodes or a message queue which means you have already begun with building a distributed system.</p>\n<p>But <strong>what do we even know about distributed systems? Are they really different to simple single server setup?</strong></p>\n<p>\n\t\t<video\n\t\t\tsrc=https://media.giphy.com/media/2u8vej0S5Mx7W/giphy.mp4\n\t\t\twidth=\"100%\"\n\t\t\theight=\"auto\"\n\t\t\tpreload=\"auto\"\n\t\t\tmuted=\"true\"\n\t\t\ttitle=\"Welcome to the real world of distributed systems\"\n\t\t\tautoplay\n\t\t\tplaysinline\n\t\t\tcontrols\n\t\t\tloop\n\t\t></video>\n\t</p>\n<p>Despite all benefits, I feel like many people <strong>underestimate challenges</strong> that come out of the box with distributed setups. So our goal is to dig into the nature of distributed systems and improve our reasoning about them.</p>\n<h2 id=\"single-server-setup\" style=\"position:relative;\"><a href=\"#single-server-setup\" aria-label=\"single server setup permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Single Server Setup</h2>\n<p>The Single server setup is when your application, database and files are served on the same physical or virtual server. It is a reasonable baseline and often you may never need more than that. We work a lot with this kind of system. Even our personal computers may be an example of the single-node systems.</p>\n<p>This is probably how we can get in a situation when we tend to apply our single server mental model to think about distributed systems. However, distributed setup is a fundamentally different beast and it has four main differences:</p>\n<ul>\n<li><strong>Latency</strong> - when we have a single server, it's <strong>CPU power and latency</strong> we worry about. In distributed systems, our main concern is <strong>network latency</strong>.</li>\n<li><strong>Memory Access</strong> - in a single server, we have access to the <strong>same memory space</strong>, so it may be easier to write programs that interact with each other. You cannot get direct access from one server to another.</li>\n<li><strong>Partial Failures</strong> - both distributed and a single node systems have components that could fail. In the case of a single server, there may be either total failure of the server (OS crashes or failures on the ISP side) or partial failures of the applications installed on the server. Every single component in a distributed setup surely shares these problems. However, they become <strong>undisguisable</strong> from each other. Having more components only <strong>increases the chance that some of them would be affected by some failures</strong>.</li>\n</ul>\n<p>These few differences are so significant that they effectively change the way we need to think and build distributed systems.</p>\n<h2 id=\"beyond-single-server\" style=\"position:relative;\"><a href=\"#beyond-single-server\" aria-label=\"beyond single server permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Beyond Single Server</h2>\n<p>From the brief comparison it becomes clear that the role of the network is crucial in multi-server setups. In order to let services communicate with each other, they expose accessible APIs via standard protocols like:</p>\n<ul>\n<li>REST</li>\n<li>SOAP</li>\n<li>RPC</li>\n</ul>\n<p>This helps to overcome memory access limitations. Communication itself happens via network links and this is where the fun part starts.</p>\n<h2 id=\"unreliable-network\" style=\"position:relative;\"><a href=\"#unreliable-network\" aria-label=\"unreliable network permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Unreliable Network</h2>\n<p>The role of the network becomes significant in the distributed systems. The problem is that the network is not a reliable thing.</p>\n<p>\n\t\t<video\n\t\t\tsrc=https://media.giphy.com/media/JzzLFXnZt4aiI/giphy.mp4\n\t\t\twidth=\"100%\"\n\t\t\theight=\"auto\"\n\t\t\tpreload=\"auto\"\n\t\t\tmuted=\"true\"\n\t\t\ttitle=\"Welcome to the real world of distributed systems\"\n\t\t\tautoplay\n\t\t\tplaysinline\n\t\t\tcontrols\n\t\t\tloop\n\t\t></video>\n\t</p>\n<p>Requests over the network can fail and there are a lot of reasons for that. Your router may just stop working because of a software or hardware issue, your load balances may drop some requests your database cluster silently or maybe <a href=\"https://techcrunch.com/2013/03/28/three-men-arrested-for-attempting-to-cut-undersea-internet-cable-in-egypt/\" target=\"_blank\" rel=\"noopener noreferrer\">someone just wants to cut your connection cable</a>. Wild nature <a href=\"https://www.theguardian.com/technology/2014/aug/14/google-undersea-fibre-optic-cables-shark-attacks\" target=\"_blank\" rel=\"noopener noreferrer\">doesn't help you either</a>.</p>\n<p>In other cases, your hardware may be partially faulty in a way that would slow down the whole nodes that it connects. So troubleshooting becomes time-consuming and painful.</p>\n<p>While people work on making networks reliable, there will probably always be a reason that causes the failure.</p>\n<p>Even if network links work well, it doesn't mean that we are safe.</p>\n<p>Network can be intercepted and the information may be <strong>recorded, modified or fabricated</strong>. This becomes an issue if transmitted information is confidential like PII or CC data.</p>\n<h2 id=\"two-generals\" style=\"position:relative;\"><a href=\"#two-generals\" aria-label=\"two generals permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Two Generals</h2>\n<p>Work in an unreliable environment is a common problem of all distributed systems. It's being generally formulated as <strong>Byzantine Generalâ€™s Problem</strong>.</p>\n<p>Imagine there are a few armies with generals that besieged the city they want to capture. In order to successfully perform the campaign, they need to attack the city at the same time. Otherwise, their resources would not be enough and the city army would defeat them.</p>\n<p>The only way generals can communicate is via messengers that have to bypass the enemy territory to deliver a message to another general.</p>\n<p>\n\t\t<video\n\t\t\tsrc=https://media.giphy.com/media/XBLppkG4DwyKnplu1v/giphy.mp4\n\t\t\twidth=\"100%\"\n\t\t\theight=\"auto\"\n\t\t\tpreload=\"auto\"\n\t\t\tmuted=\"true\"\n\t\t\ttitle=\"Consensus\"\n\t\t\tautoplay\n\t\t\tplaysinline\n\t\t\tcontrols\n\t\t\tloop\n\t\t></video>\n\t</p>\n<p>The situation is complicated as messengers can be intercepted and the message can be fabricated. Also, both messengers and generals can be traitors that are the objective to fail the operation and save the city.</p>\n<p>Generals and messengers need to agree on all messages that they send to each other in order to win the battle. This agreement is called <strong>consensus</strong>.</p>\n<p>In a context of distributed systems, the consensus is important to synchronize the state of the system when it's distributed among several nodes.</p>\n<h2 id=\"latency-bandwidth-and-transport-cost\" style=\"position:relative;\"><a href=\"#latency-bandwidth-and-transport-cost\" aria-label=\"latency bandwidth and transport cost permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Latency, Bandwidth and Transport Cost</h2>\n<p>When two applications communicate on the same server, the communication latency is super small and close to zero. In contrast, communication over the network is a different story.</p>\n<p>\n\t\t<video\n\t\t\tsrc=https://media.giphy.com/media/6xE1FNcorRInS/giphy.mp4\n\t\t\twidth=\"100%\"\n\t\t\theight=\"auto\"\n\t\t\tpreload=\"auto\"\n\t\t\tmuted=\"true\"\n\t\t\ttitle=\"Latency in Matrix\"\n\t\t\tautoplay\n\t\t\tplaysinline\n\t\t\tcontrols\n\t\t\tloop\n\t\t></video>\n\t</p>\n<p>Network latency can be <strong>a few orders of magnitude higher</strong> than communication inside of the same memory space. Two computers from different parts of the world are connected via optical cables and the light is used to transmit information. So we seem to be pretty much limited to the speed of the light at this point. All of this tells us that latency is a new and important factor in distributed systems.</p>\n<p>Another resource associated with communication over the network is <strong>bandwidth</strong>. Each network connection has a bandwidth limit and you cannot send a larger chunk of data over a unit of time via the network. This is especially important when you deal with file uploading and downloading like in Google Drive, Dropbox or even Netflix where you need to stream high-quality video files for millions of users over the globe.</p>\n<p>Latency and bandwidth are two main costs of information transportation. However, there are other ones. For example, you will definitely pay for the whole <strong>network infrastructure</strong> needed to make your communication possible. Even if that cost is hidden in the cloud provider tariffs.</p>\n<p>Yet another cost is the CPU resource you spend on <strong>serializing and deserializing</strong> data as well as <strong>the cost of TCP and HTTPS handshakes</strong>. This is the cost we need to pay in order to transfer data between transportation to application layers. The same story goes for when we decide to <strong>compress and decompress</strong> data in order to speed up data transmission and optimize bandwidth.</p>\n<p>In order to imagine all of this, let's take a look at <a href=\"http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">some measurements</a> that were done in Google data centers a while ago. They still are useful when you compare them:</p>\n<ul>\n<li>Main memory reference - 100ns</li>\n<li>Compress 1KB with Zippy - 10,000ns (10 us)</li>\n<li>Read 1 MB sequentially from memory - 250,000ns (250us)</li>\n<li>Round trip within the same datacenter - 500,000ns (500 us)</li>\n<li>Read 1 MB sequentially from 1 Gbps network - 10,000,000ns (10,000 us or 10 ms)</li>\n<li>Read 1 MB sequentially from HDD - 30,000,000ns (30,000us or 30 ms)</li>\n<li>Send packet CA -> Netherlands -> CA - 150,000,000ns (150,000us or 150ms)</li>\n</ul>\n<p>This may be counterintuitive, but in some cases, <strong>network communication may be faster than reading from disk</strong>. Above we see that reading 1MB of data is a few times faster from 1Gbps network than from HDD.</p>\n<h2 id=\"cap-theorem\" style=\"position:relative;\"><a href=\"#cap-theorem\" aria-label=\"cap theorem permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>CAP Theorem</h2>\n<p>Network hardware and software are not the only unreliable components. We can easily imagine a situation when some of the connected nodes become faulty because of a broken hard drive or RAM stick. In such cases, there is a part of the distributed system that still works and another part that is failed. These situations are just inevitable.</p>\n<p>What can we do about it? We need to manage such failures and build system architectures around this fundamental problem. Such systems are called <strong>fault-tolerant</strong>.</p>\n<p><strong>CAP theorem</strong> says that there are two logical ways to respond to the partial failure of the system:</p>\n<ul>\n<li>keep part of the system outdated but <strong>available</strong> for other clients</li>\n<li>lock the system until all nodes become <strong>consistent</strong> after the failure</li>\n</ul>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 90.58823529411765%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsSAAALEgHS3X78AAADaklEQVQ4y32T6VMaZxzH+eM6fZO+6jQzzUybFx0nL5zJAVNR6sG1hSiHyyGKkLZiTEhCRDlkQe5VlmM5VlkDgrRyOYlWWCHcu3QJfdMx6fP85je/5/g8399zMYafKyWi5Cw4X+AvDEcGS96CX+J9sn97GuN2F1pFpR4pS89irY2Muc7k7/ItWUur36JHKbrehkmKpH3mMqNElNqoVg2rFQGFMqgEfeDKwYoirIBy0BdhutC5bWFbq7FVGljcW5RAEtm+jPZyj3w9tq6OqMtEecRT1H/gcfuicaGJaHRxncwlA8wAsAOIrCLeG57YJtbH9XRGSBH5Inx2fbYaWdGhWhWs5psENEwvsWBckLtluoReGVZ7Ct7/U16JarWoXouoQLcE9I/2LPUsgwdrelSnQcBw6XPKA5I6vuq9yt1MegITLujnoEMefflbQruWeGZANXsBtt/5k9d2731cTlX8VLc+VhzB7T75NtcURq8XUWIKzn+76/zmjzf3rN5fYMtLZOnA9r1z42vH5lc49GM7zGuFZtuJZfKm+K+yvdgTp1oqvKM86SrxLtOfvmuyfGfzTFjtzrf3kZ07kPFO0v5DLwkMsSUKk1AJ4eAIpLoEo9waTlsx5pbvscH9YNX85LlvOdXgRooT/jRoXHQ8uxuD7lfhR3k7+3Bj0qt/ENqcDG5M5i1MMv+aAVe78/4S333KdWXmoRPayxJ1EPuoSNacLtBvnbkMAcPUEnEoqLhnzyFO0cn5c2+66p7roQADKrZ5cOXX4DkQLIpDVSBYEgb+Ekev1MkPjh3h698fVg6EdZhLhPj1Q34rAjQQYQMR0EEHFTEC1R7HlZs2hecc+MxunGNNTptjAFxWHdVt1iW7kfk+BBShmbSZhZme5GxTdBB/9ag8Vj4j+uLYNW8/Q5Oz9iOanIdwWbImwTpmnzFqZ7fjT6ueWXybdf5pifQ282R3qhacG2QNo9M2FdpcuCpwnwp8ZwJvXhK7AjFCmmoqIoVCAOhFBTn7TNbKPrWxM5ap/B4H33584eUOO3+PYKJLbmZb4kRTkmpKsaYkQTyN10Wx2n6FpGqZTkzUQubbEX47IhhZeKGDigfX70b3PH5pnQEVuuhsvGusp290+I0x9zFb6w0//T6q9aFf2Okda7qYqnusoWO6Z/zC/gEH1khJif8IuAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"CAP Theorem Visualized\" title=\"CAP Theorem Visualized\" src=\"/static/06bd45aef2a72b7854e9797f2db0e4c2/c5bb3/cap-theorem.png\" srcset=\"/static/06bd45aef2a72b7854e9797f2db0e4c2/04472/cap-theorem.png 170w,\n/static/06bd45aef2a72b7854e9797f2db0e4c2/9f933/cap-theorem.png 340w,\n/static/06bd45aef2a72b7854e9797f2db0e4c2/c5bb3/cap-theorem.png 680w,\n/static/06bd45aef2a72b7854e9797f2db0e4c2/ae694/cap-theorem.png 850w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n          </p>\n<div class=\"image-title\">CAP Theorem Visualized (<a target=\"_blank\" rel=\"noopener\" href=\"https://www.researchgate.net/figure/Visualization-of-CAP-theorem_fig2_282679529\">source</a>)</div>\n<p>The most important is that <strong>we cannot pick all three of these attributes: Consistency, Availability and Partial Failure Tolerance</strong>.</p>\n<p>Another interesting thing is the <strong>Consistency-Availability</strong> pair. Since partial failures are unavoidable, it's effectively not possible to implement this pair for distributed systems and it should be only possible in the case of a single-server setup.</p>\n<p>However, failures don't happen all the time and most of the time the system is in a fully functional state. That's why there is an addition to the CAP theorem called the <strong>PACELC theorem</strong>.</p>\n<p>PACELC theorem states that when there are <strong>no partial failures</strong> in the system, then a distributed system can <strong>trade between latency and consistency</strong>. In other words, in case you have a horizontally scaled cluster with a couple of nodes, then you can balance your system latency over consistency by choosing the number of nodes you need to have in sync when change happens. The change may be propagated with <strong>a few nodes right away</strong> and respond to the client after that. The rest of the nodes eventually become synced with this change over time. On the flip side, you can wait until <strong>all nodes</strong> get in sync and only then respond to the client. In the second case, we get a <strong>strongly consistent</strong> system, but with a <strong>higher latency</strong> comparing to the first case.</p>\n<p>By choosing the number of nodes we want to have in sync when changes happen, we can specify <strong>the level of consistency</strong> of the system.</p>\n<h2 id=\"at-least-once-delivery\" style=\"position:relative;\"><a href=\"#at-least-once-delivery\" aria-label=\"at least once delivery permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>At Least Once Delivery</h2>\n<p>Yet another interesting implication of partial failures is that it's hard to <strong>deliver messages exactly once every time</strong>.</p>\n<p>When a client dispatches a message it transmits to the receiver, the receiver persists the message and respond by an acknowledge message back. This is a normal flow.</p>\n<p>Let's make it real by adding a partial failure. Now when the client sends a message and doesn't get a response in a reasonable amount of time. Maybe the receiver is down? Or there was a network failure? Or maybe the receiver is just overloaded at this time and slow down with response?</p>\n<p>There is no way to distinguish these issues. The message we try to send may be really important like an order fulfilment request. We are better to be safe than sorry. So the client sends out one more message and waits again. We do this until we are sure that the message is received e.g. when we receive an ack message in response.</p>\n<p>However, at that point, the receiver may get <strong>several duplicated messages</strong>.</p>\n<p>\n\t\t<video\n\t\t\tsrc=https://media.giphy.com/media/l41lRvFQYdlfvDTLG/giphy.mp4\n\t\t\twidth=\"100%\"\n\t\t\theight=\"auto\"\n\t\t\tpreload=\"auto\"\n\t\t\tmuted=\"true\"\n\t\t\ttitle=\"At least once delivery\"\n\t\t\tautoplay\n\t\t\tplaysinline\n\t\t\tcontrols\n\t\t\tloop\n\t\t></video>\n\t</p>\n<p>For this reason, we need to make sure that message workers have business logic that would not make the system state inconsistent in case of message duplications (e.g. customer is being charged a few times for the same order). Sometimes this property is called <strong>idempotency</strong>.</p>\n<h2 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Summary</h2>\n<p>I hope you noticed that designing distributed systems is a good fit for pessimists, because you always need to expect that system could partially fail at some point.</p>\n<p>Other than that, distributed systems have inherited complexity associated with their distributed nature. That's why you need to go building such a system if they are absolutely needed.</p>\n<p>The distributed world is all about tradeoffs. You need to know and realize them to successfully build robust distributed systems.</p>\n<p>\n\t\t<video\n\t\t\tsrc=https://media.giphy.com/media/3o7btNhMBytxAM6YBa/giphy.mp4\n\t\t\twidth=\"100%\"\n\t\t\theight=\"auto\"\n\t\t\tpreload=\"auto\"\n\t\t\tmuted=\"true\"\n\t\t\ttitle=\"Summary\"\n\t\t\tautoplay\n\t\t\tplaysinline\n\t\t\tcontrols\n\t\t\tloop\n\t\t></video>\n\t</p>\n<h2 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>References</h2>\n<ul>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://www.infoq.com/presentations/distributed-systems-complexity-human-factor/\">Why Distributed Systems Are Hard</a></li>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://dl.acm.org/doi/pdf/10.5555/974938\">A Note on Distributed Computing</a></li>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://bravenewgeek.com/you-cannot-have-exactly-once-delivery/\">You Cannot Have Exactly-Once Delivery</a></li>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://bravenewgeek.com/understanding-consensus/\">Understanding Consensus</a></li>\n<li><a target=\"_blank\" rel=\"noopener\" href=\"https://levelup.gitconnected.com/practical-understanding-of-flp-impossibility-for-distributed-consensus-8886e73cdfe5\">Practical Understanding of FLP Impossibility for Distributed Consensus</a></li>\n</ul>","timeToRead":9,"rawMarkdownBody":"\nThe Internet evolves rapidly. We started  our own hardware servers, moved to managed hostings and now we have entered the era of cloud computing. The ever growing need to handle more and more traffic pushes us beyond the limits.\n\nAs a result, it's so natural nowadays to spin off a few instances on AWS or GCP and start building one's own SaaS service that will scale in a way we need. Perhaps, your infrastructure already has load balancing, a couple of nodes or a message queue which means you have already begun with building a distributed system.\n\nBut **what do we even know about distributed systems? Are they really different to simple single server setup?**\n\n`video: title: \"Welcome to the real world of distributed systems\": https://media.giphy.com/media/2u8vej0S5Mx7W/giphy.mp4`\n\nDespite all benefits, I feel like many people **underestimate challenges** that come out of the box with distributed setups. So our goal is to dig into the nature of distributed systems and improve our reasoning about them.\n\n## Single Server Setup\n\nThe Single server setup is when your application, database and files are served on the same physical or virtual server. It is a reasonable baseline and often you may never need more than that. We work a lot with this kind of system. Even our personal computers may be an example of the single-node systems.\n\nThis is probably how we can get in a situation when we tend to apply our single server mental model to think about distributed systems. However, distributed setup is a fundamentally different beast and it has four main differences:\n\n- **Latency** - when we have a single server, it's **CPU power and latency** we worry about. In distributed systems, our main concern is **network latency**.\n- **Memory Access** - in a single server, we have access to the **same memory space**, so it may be easier to write programs that interact with each other. You cannot get direct access from one server to another.\n- **Partial Failures** - both distributed and a single node systems have components that could fail. In the case of a single server, there may be either total failure of the server (OS crashes or failures on the ISP side) or partial failures of the applications installed on the server. Every single component in a distributed setup surely shares these problems. However, they become **undisguisable** from each other. Having more components only **increases the chance that some of them would be affected by some failures**.\n\nThese few differences are so significant that they effectively change the way we need to think and build distributed systems.\n\n## Beyond Single Server\n\nFrom the brief comparison it becomes clear that the role of the network is crucial in multi-server setups. In order to let services communicate with each other, they expose accessible APIs via standard protocols like:\n\n- REST\n- SOAP\n- RPC\n\nThis helps to overcome memory access limitations. Communication itself happens via network links and this is where the fun part starts.\n\n## Unreliable Network\n\nThe role of the network becomes significant in the distributed systems. The problem is that the network is not a reliable thing.\n\n`video: title: \"Welcome to the real world of distributed systems\": https://media.giphy.com/media/JzzLFXnZt4aiI/giphy.mp4`\n\nRequests over the network can fail and there are a lot of reasons for that. Your router may just stop working because of a software or hardware issue, your load balances may drop some requests your database cluster silently or maybe [someone just wants to cut your connection cable](https://techcrunch.com/2013/03/28/three-men-arrested-for-attempting-to-cut-undersea-internet-cable-in-egypt/). Wild nature [doesn't help you either](https://www.theguardian.com/technology/2014/aug/14/google-undersea-fibre-optic-cables-shark-attacks).\n\nIn other cases, your hardware may be partially faulty in a way that would slow down the whole nodes that it connects. So troubleshooting becomes time-consuming and painful.\n\nWhile people work on making networks reliable, there will probably always be a reason that causes the failure.\n\nEven if network links work well, it doesn't mean that we are safe.\n\nNetwork can be intercepted and the information may be **recorded, modified or fabricated**. This becomes an issue if transmitted information is confidential like PII or CC data.\n\n## Two Generals\n\nWork in an unreliable environment is a common problem of all distributed systems. It's being generally formulated as **Byzantine Generalâ€™s Problem**.\n\nImagine there are a few armies with generals that besieged the city they want to capture. In order to successfully perform the campaign, they need to attack the city at the same time. Otherwise, their resources would not be enough and the city army would defeat them.\n\nThe only way generals can communicate is via messengers that have to bypass the enemy territory to deliver a message to another general.\n\n`video: title: \"Consensus\": https://media.giphy.com/media/XBLppkG4DwyKnplu1v/giphy.mp4`\n\nThe situation is complicated as messengers can be intercepted and the message can be fabricated. Also, both messengers and generals can be traitors that are the objective to fail the operation and save the city.\n\nGenerals and messengers need to agree on all messages that they send to each other in order to win the battle. This agreement is called **consensus**.\n\nIn a context of distributed systems, the consensus is important to synchronize the state of the system when it's distributed among several nodes.\n\n## Latency, Bandwidth and Transport Cost\n\nWhen two applications communicate on the same server, the communication latency is super small and close to zero. In contrast, communication over the network is a different story.\n\n`video: title: \"Latency in Matrix\": https://media.giphy.com/media/6xE1FNcorRInS/giphy.mp4`\n\nNetwork latency can be **a few orders of magnitude higher** than communication inside of the same memory space. Two computers from different parts of the world are connected via optical cables and the light is used to transmit information. So we seem to be pretty much limited to the speed of the light at this point. All of this tells us that latency is a new and important factor in distributed systems.\n\nAnother resource associated with communication over the network is **bandwidth**. Each network connection has a bandwidth limit and you cannot send a larger chunk of data over a unit of time via the network. This is especially important when you deal with file uploading and downloading like in Google Drive, Dropbox or even Netflix where you need to stream high-quality video files for millions of users over the globe.\n\nLatency and bandwidth are two main costs of information transportation. However, there are other ones. For example, you will definitely pay for the whole **network infrastructure** needed to make your communication possible. Even if that cost is hidden in the cloud provider tariffs.\n\nYet another cost is the CPU resource you spend on **serializing and deserializing** data as well as **the cost of TCP and HTTPS handshakes**. This is the cost we need to pay in order to transfer data between transportation to application layers. The same story goes for when we decide to **compress and decompress** data in order to speed up data transmission and optimize bandwidth.\n\nIn order to imagine all of this, let's take a look at [some measurements](http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf) that were done in Google data centers a while ago. They still are useful when you compare them:\n\n- Main memory reference - 100ns\n- Compress 1KB with Zippy - 10,000ns (10 us)\n- Read 1 MB sequentially from memory - 250,000ns (250us)\n- Round trip within the same datacenter - 500,000ns (500 us)\n- Read 1 MB sequentially from 1 Gbps network - 10,000,000ns (10,000 us or 10 ms)\n- Read 1 MB sequentially from HDD - 30,000,000ns (30,000us or 30 ms)\n- Send packet CA -> Netherlands -> CA - 150,000,000ns (150,000us or 150ms)\n\nThis may be counterintuitive, but in some cases, **network communication may be faster than reading from disk**. Above we see that reading 1MB of data is a few times faster from 1Gbps network than from HDD.\n\n## CAP Theorem\n\nNetwork hardware and software are not the only unreliable components. We can easily imagine a situation when some of the connected nodes become faulty because of a broken hard drive or RAM stick. In such cases, there is a part of the distributed system that still works and another part that is failed. These situations are just inevitable.\n\nWhat can we do about it? We need to manage such failures and build system architectures around this fundamental problem. Such systems are called **fault-tolerant**.\n\n**CAP theorem** says that there are two logical ways to respond to the partial failure of the system:\n\n- keep part of the system outdated but **available** for other clients\n- lock the system until all nodes become **consistent** after the failure\n\n![CAP Theorem Visualized](./img/cap-theorem.png \"CAP Theorem Visualized\")\n<div class=\"image-title\">CAP Theorem Visualized (<a target=\"_blank\" rel=\"noopener\" href=\"https://www.researchgate.net/figure/Visualization-of-CAP-theorem_fig2_282679529\">source</a>)</div>\n\nThe most important is that **we cannot pick all three of these attributes: Consistency, Availability and Partial Failure Tolerance**.\n\nAnother interesting thing is the **Consistency-Availability** pair. Since partial failures are unavoidable, it's effectively not possible to implement this pair for distributed systems and it should be only possible in the case of a single-server setup.\n\nHowever, failures don't happen all the time and most of the time the system is in a fully functional state. That's why there is an addition to the CAP theorem called the **PACELC theorem**.\n\nPACELC theorem states that when there are **no partial failures** in the system, then a distributed system can **trade between latency and consistency**. In other words, in case you have a horizontally scaled cluster with a couple of nodes, then you can balance your system latency over consistency by choosing the number of nodes you need to have in sync when change happens. The change may be propagated with **a few nodes right away** and respond to the client after that. The rest of the nodes eventually become synced with this change over time. On the flip side, you can wait until **all nodes** get in sync and only then respond to the client. In the second case, we get a **strongly consistent** system, but with a **higher latency** comparing to the first case.\n\nBy choosing the number of nodes we want to have in sync when changes happen, we can specify **the level of consistency** of the system.\n\n## At Least Once Delivery\n\nYet another interesting implication of partial failures is that it's hard to **deliver messages exactly once every time**.\n\nWhen a client dispatches a message it transmits to the receiver, the receiver persists the message and respond by an acknowledge message back. This is a normal flow.\n\nLet's make it real by adding a partial failure. Now when the client sends a message and doesn't get a response in a reasonable amount of time. Maybe the receiver is down? Or there was a network failure? Or maybe the receiver is just overloaded at this time and slow down with response?\n\nThere is no way to distinguish these issues. The message we try to send may be really important like an order fulfilment request. We are better to be safe than sorry. So the client sends out one more message and waits again. We do this until we are sure that the message is received e.g. when we receive an ack message in response.\n\nHowever, at that point, the receiver may get **several duplicated messages**.\n\n`video: title: \"At least once delivery\": https://media.giphy.com/media/l41lRvFQYdlfvDTLG/giphy.mp4`\n\nFor this reason, we need to make sure that message workers have business logic that would not make the system state inconsistent in case of message duplications (e.g. customer is being charged a few times for the same order). Sometimes this property is called **idempotency**.\n\n## Summary\n\nI hope you noticed that designing distributed systems is a good fit for pessimists, because you always need to expect that system could partially fail at some point.\n\nOther than that, distributed systems have inherited complexity associated with their distributed nature. That's why you need to go building such a system if they are absolutely needed.\n\nThe distributed world is all about tradeoffs. You need to know and realize them to successfully build robust distributed systems.\n\n`video: title: \"Summary\": https://media.giphy.com/media/3o7btNhMBytxAM6YBa/giphy.mp4`\n\n## References\n\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://www.infoq.com/presentations/distributed-systems-complexity-human-factor/\">Why Distributed Systems Are Hard</a>\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://dl.acm.org/doi/pdf/10.5555/974938\">A Note on Distributed Computing</a>\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://bravenewgeek.com/you-cannot-have-exactly-once-delivery/\">You Cannot Have Exactly-Once Delivery</a>\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://bravenewgeek.com/understanding-consensus/\">Understanding Consensus</a>\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://levelup.gitconnected.com/practical-understanding-of-flp-impossibility-for-distributed-consensus-8886e73cdfe5\">Practical Understanding of FLP Impossibility for Distributed Consensus</a>","wordCount":{"words":1942},"frontmatter":{"path":"/blog/nature-of-distributed-systems/","humanDate":"Jul 20, 2021","fullDate":"2021-07-20","title":"The Nature of Distributed Systems","keywords":["distributed system design"],"includeMath":null,"excerpt":"How distributed systems are different to single-node setups?","cover":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMEAgX/xAAWAQEBAQAAAAAAAAAAAAAAAAACAAH/2gAMAwEAAhADEAAAAWb5ldjCcK//xAAcEAACAQUBAAAAAAAAAAAAAAABAgADERIhIjH/2gAIAQEAAQUCtzkYaqiI0HhXf//EABURAQEAAAAAAAAAAAAAAAAAAAAh/9oACAEDAQE/AUf/xAAWEQEBAQAAAAAAAAAAAAAAAAAAITH/2gAIAQIBAT8BxX//xAAaEAACAgMAAAAAAAAAAAAAAAAAARAhETGh/9oACAEBAAY/AsotdLnZ/8QAGxAAAwACAwAAAAAAAAAAAAAAAAERITFBYcH/2gAIAQEAAT8hrk+ilhQbSqOj6H2Ls8D/2gAMAwEAAgADAAAAEDTf/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAR/9oACAEDAQE/EBGyf//EABcRAQEBAQAAAAAAAAAAAAAAAAEAEUH/2gAIAQIBAT8QRm83/8QAHRABAAMAAgMBAAAAAAAAAAAAAQARITFhQVGBwf/aAAgBAQABPxC4MX0OHp1GWQwos78iWiOQt/IcbWubHdNPcUxW8BP/2Q==","aspectRatio":1.5017667844522968,"src":"/static/95ef24b3227ac9169954fd4efee37f14/35aef/marcin-jozwiak-oh0DITWoHi4-unsplash.jpg","srcSet":"/static/95ef24b3227ac9169954fd4efee37f14/809fc/marcin-jozwiak-oh0DITWoHi4-unsplash.jpg 850w,\n/static/95ef24b3227ac9169954fd4efee37f14/4f4f6/marcin-jozwiak-oh0DITWoHi4-unsplash.jpg 1700w,\n/static/95ef24b3227ac9169954fd4efee37f14/35aef/marcin-jozwiak-oh0DITWoHi4-unsplash.jpg 3400w,\n/static/95ef24b3227ac9169954fd4efee37f14/9a201/marcin-jozwiak-oh0DITWoHi4-unsplash.jpg 5100w,\n/static/95ef24b3227ac9169954fd4efee37f14/d8e16/marcin-jozwiak-oh0DITWoHi4-unsplash.jpg 5464w","sizes":"(max-width: 3400px) 100vw, 3400px"}}},"coverCredits":"Photo by <a href=\"https://unsplash.com/@marcinjozwiak\">Marcin Jozwiak</a> on <a href=\"https://unsplash.com/s/photos/distribution-center\">Unsplash</a>"},"parent":{"__typename":"File","fields":{"gitLogLatestDate":"2021-12-30 11:50:02 +0200"}}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/communicate-through-code/","title":"Communicate Through Code"}},"nextThought":{"frontmatter":{"path":"/blog/design-lru-cache/","title":"Design LRU Cache"}}}}}