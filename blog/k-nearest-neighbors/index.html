<!DOCTYPE html><html lang="en" class="blogpost-view-page"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><link rel="stylesheet" href="/styles.7abeaee38f3b2449c649.css"/><title data-react-helmet="true">K-Nearest Neighbors üë®‚Äçüë©‚Äçüëß‚Äçüë¶ - Blog by Roman Glushko</title><meta data-react-helmet="true" name="image" property="og:image" content="https://www.romaglushko.com/static/dab856139d3de9754d8bc45de82e7772/35aef/pexels-ollie-craig-6398503-min.jpg"/><meta data-react-helmet="true" name="description" property="og:description" content="Theoretical aspects of the KNN algorithm. Where it can be applied and when it fails."/><meta data-react-helmet="true" name="keywords" content="machine learning"/><meta data-react-helmet="true" name="author" content="@roma_glushko"/><meta data-react-helmet="true" property="og:title" content="K-Nearest Neighbors üë®‚Äçüë©‚Äçüëß‚Äçüë¶ - Blog"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" name="twitter:creator" content="@roma_glushko"/><meta data-react-helmet="true" name="twitter:title" content="K-Nearest Neighbors üë®‚Äçüë©‚Äçüëß‚Äçüë¶ - Blog"/><meta data-react-helmet="true" name="twitter:description" content="Theoretical aspects of the KNN algorithm. Where it can be applied and when it fails."/><meta data-react-helmet="true" property="og:url" content="https://www.romaglushko.com/blog/k-nearest-neighbors/"/><script data-react-helmet="true" type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","image":"/static/dab856139d3de9754d8bc45de82e7772/35aef/pexels-ollie-craig-6398503-min.jpg","headline":"K-Nearest Neighbors üë®‚Äçüë©‚Äçüëß‚Äçüë¶","dateCreated":"2021-04-05","datePublished":"2021-04-05","dateModified":"2021-04-05","inLanguage":"en-US","isFamilyFriendly":"true","author":{"@type":"Person","name":"Roman Glushko"},"publisher":{"@type":"Organization","name":"Roman Glushko's Website"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.romaglushko.com/blog/k-nearest-neighbors/"},"keywords":["machine learning"],"genre":["machine learning","software engineering","science","deep learning","statistics"],"articleSection":"Technical Blog","articleBody":"\nK-Nearest Neighbors (a.k.a KNN) is one of the famous machine learning approaches. It's simple, intuitive and useful tool to have in your machine learning toolbox.\n\nKNN is instance-based learning algorithm. It doesn't make any assumption about dataset relations and uses provided samples in order to estimate new items.\n\n## Intuition\n\nAs old saying goes, you are who you surround yourself with. This idea even has mathematical interpretation. In N-dimensional space, points that are closer to each other are more similar, then ones that stand apart. This is the assumption that KNN algorithm makes. It tries to find K nearest known points or samples, also called \"neighbors\", to some unknown sample. Majority class of neighbors defines class of the unknown sample.\n\n![Nearest neighbors defines class of unknown sample](./img/k-nearest-neighbors.png \"Nearest neighbors defines class of unknown sample\")\n<div class=\"image-title\">Nearest neighbors defines class of unknown sample</div>\n\nTo came from this idea to implementation, we need to define how to find closest points.\n\n## Closest Points\n\nWe need to somehow measure distance between samples in order to find closest (hence similar) neighbors. Turned out there are quite a few metrics we can use:\n\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Euclidean Distance</a> - straight-line distance between two points\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Taxicab_geometry\">Manhattan (City Block) Distance</a> - distance between two points traversed along one axis at the time at right angle\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Minkowski_distance\">Minkowski Distance</a> - Generalization of Euclidean and Manhattan distances\n- scikit-learn supports <a target=\"_blank\" rel=\"noopener\" href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric\">other metrics</a> as well\n\n![Different distance metrics and their boundaries](./img/distance-metrics.png?22 \"Different distance metrics and their boundaries\")\n<div class=\"image-title\">Distance metrics and their boundaries in 2D</div>\n\nEuclidean and Manhattan distances are two the most popular distance metrics:\n\n$$\nD(x, y)_{euclidean} = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}\n$$\n\n$$\nD(x, y)_{manhattan} = \\sum_{i=1}^n |x_i-y_i|\n$$\n\nwhere $x=(x_1, x_2, ..., x_n)$ and $y=(y_1, y_2, ..., y_n)$ are two points in the N-dimensional space.\n\nMinkowski distance incorporates both Euclidean (when p is 2) and Manhattan (when p is 1) distances and gives additional degree of flexibility via p param:\n\n$$\nD(x, y)_{minkowski} = \\left(\\sum_{i=1}^n (|x_i-y_i|)^p\\right)^\\frac{1}{p}\n$$\n\nThese distance metrics are intuitive and fast to compute.\n\nHowever, all features or dimensions are treated the same way. This may be an issue if features have values in different scales. For example,\n\n- customer age value is normally between 18 and 100\n- annual income values lies between 20,000 and 200,000 (and sometimes goes far beyond)\n\nWhen calculating distance metrics, this leads to a situation when major part of similarity actually comes from large scale-value features (like annual income). As a result, the rest of the features will be almost neglected and won't be really taken into account during similarity checking.\n\n## Standardization\n\nIgnoring small-scale features in the similarity checking is rarely a desired effect. We would like rather to avoid it to get real neighbors of our sample.\n\nWe can try to rescale all features to the same range (0-1, for example). This will standardize or normalize features and let them have the same influence in the distance calculations.\n\n$$\nZ = \\frac{x - \\overline{x}}{S}\n$$\nwhere $x$ is sample feature value, $\\overline{x}$ is a feature mean and $S$ is a feature standard deviation.\n\nZ is a <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Standard_score\">standard score (or z-score)</a> of the sample feature value. It's a dimensionless quantity and it shows how \"unusual\" or far sample feature value from the feature mean (which is usual or expected value).\n\nThough normalization changes values of features, it doesn't change feature distribution shape.\n\n## Dense Dataset\n\nWe assume that similar samples are close in the N-dimensional feature space. Is this always a case?\n\nTurned out that with growing number of dimensions (or features), number of samples should grow exponentially in order to keep the same density of sample points in the space. High-dimensional space contains far more actual space between points than our intuition may suggest.\n\nThis may lead to a situation when distance to neighbors across all feature axis are just slightly bigger than average distance between points. In sparse high-dimensional feature space we are loosing our notion of nearest points.\n\nThis is the reason why ratio of features to samples is important for KNN algorithm.\n\n## Neighborhood Size\n\nEffectiveness of applying KNN depends greatly on choosing appropriate size of neighborhood a.k.a **K** hyperparam. In turn this choice is determined by dataset. It's usual to see K in range from 1 to 20 (5 is default size in scikit-learn).\n\nSmaller value of K adds more sensitivity and variance. For noisy datasets this means that given a slightly adjusted dataset we may get a different neighbors.  \n\nIncreasing K hyperparam adds more bias to KNN results and smoother decision boundaries. If K param value is too big, KNN will loose its ability to find local structures in the dataset.\n\nIn general, neighborhood size is subject of hyperparam tuning. Final decision should be validated on separated validation dataset.\n\n## Applications\n\nYou may find KNN-based classificators and regressors which is a trivial application of KNN idea. Let's review a few less obvious applications where KNN shines even better.\n\n### Feature Engineering\n\nKNN is able to identify local structures in the dataset. This information is useful clue itself and local structure labels may be used in classification/regression pipeline as an additional feature.\n### Neighbors in Embeddings\n\nKNN algorithm becomes useful when you have embeddings - samples projected into lower dimensional latent space. In embedding space distance means similarity and KNN can suggest similar items. This is especially useful for recommendation systems. In that case, nearest neighbor search can be optimized further by leveraging <a target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Locality-sensitive_hashing\">locality-sensitive hashing</a>, for instance.\n\n### Anomaly Detection\n\nDistances to K nearest neighbors can be viewed as an estimation of local density. If distance is relatively large, this implies that density is low and sample is likely to be an outlier.\n\n## Summary\n\nI hope we shed a bit of light on theoretical aspects of K-Nearest Neighbor algorithm and proved that this pure idea is still useful after decades of being formulated.\n\n## References\n\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/\">Practical Statistics for Data Scientists by Peter Bruce and Andrew Bruce</a>\n- <a target=\"_blank\" rel=\"noopener\" href=\"https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d\">k-Nearest Neighbors and the Curse of Dimensionality</a>","wordcount":963}</script><script data-react-helmet="true" type="application/ld+json">{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://www.romaglushko.com/"},{"@type":"ListItem","position":2,"name":"Blog","item":"https://www.romaglushko.com/blog/"},{"@type":"ListItem","position":3,"name":"K-Nearest Neighbors üë®‚Äçüë©‚Äçüëß‚Äçüë¶","item":"https://www.romaglushko.com/blog/k-nearest-neighbors/"}]}</script><script data-react-helmet="true" type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer="" async=""></script><link crossorigin="" href="https://utteranc.es" rel="preconnect"/><link crossorigin="" href="https://www.google-analytics.com" rel="preconnect"/><link rel="icon" href="/favicon-32x32.png?v=58a31253e5b93c1d7e74e8a017ff47b3"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="theme-color" content="#ffffff"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=58a31253e5b93c1d7e74e8a017ff47b3"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=58a31253e5b93c1d7e74e8a017ff47b3"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=58a31253e5b93c1d7e74e8a017ff47b3"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=58a31253e5b93c1d7e74e8a017ff47b3"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=58a31253e5b93c1d7e74e8a017ff47b3"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=58a31253e5b93c1d7e74e8a017ff47b3"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=58a31253e5b93c1d7e74e8a017ff47b3"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=58a31253e5b93c1d7e74e8a017ff47b3"/><link rel="preload" as="font" type="font/woff2" crossorigin="anonymous" href="/static/webfonts/s/dancingscript/v16/If2cXTr6YS-zF4S-kcSWSVi_sxjsohD9F50Ruu7BMSo3Sup8.woff2"/><link rel="preload" as="font" type="font/woff2" crossorigin="anonymous" href="/static/webfonts/s/dancingscript/v16/If2cXTr6YS-zF4S-kcSWSVi_sxjsohD9F50Ruu7B1i03Sup8.woff2"/><link rel="preload" as="font" type="font/woff2" crossorigin="anonymous" href="/static/webfonts/s/ledger/v11/j8_q6-HK1L3if_sBnMrx.woff2"/><style>@font-face{font-family:Dancing Script;font-style:normal;font-weight:400;font-display:swap;src:url(/static/webfonts/s/dancingscript/v16/If2cXTr6YS-zF4S-kcSWSVi_sxjsohD9F50Ruu7BMSo3Sup8.woff2) format("woff2")}@font-face{font-family:Dancing Script;font-style:normal;font-weight:700;font-display:swap;src:url(/static/webfonts/s/dancingscript/v16/If2cXTr6YS-zF4S-kcSWSVi_sxjsohD9F50Ruu7B1i03Sup8.woff2) format("woff2")}@font-face{font-family:Dancing Script;font-style:normal;font-weight:400;font-display:swap;src:url(/static/webfonts/s/dancingscript/v16/If2cXTr6YS-zF4S-kcSWSVi_sxjsohD9F50Ruu7BMSo3Sup6.woff) format("woff")}@font-face{font-family:Dancing Script;font-style:normal;font-weight:700;font-display:swap;src:url(/static/webfonts/s/dancingscript/v16/If2cXTr6YS-zF4S-kcSWSVi_sxjsohD9F50Ruu7B1i03Sup6.woff) format("woff")}@font-face{font-family:Ledger;font-style:normal;font-weight:400;font-display:swap;src:url(/static/webfonts/s/ledger/v11/j8_q6-HK1L3if_sBnMrx.woff2) format("woff2")}@font-face{font-family:Ledger;font-style:normal;font-weight:400;font-display:swap;src:url(/static/webfonts/s/ledger/v11/j8_q6-HK1L3if_sBnMr3.woff) format("woff")}</style><link rel="preconnect dns-prefetch" href="https://www.google-analytics.com"/><link rel="stylesheet"/><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="canonical" href="https://www.romaglushko.com/blog/k-nearest-neighbors/" data-baseprotocol="https:" data-basehost="www.romaglushko.com"/><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><link as="script" rel="preload" href="/webpack-runtime-b8401f474d461e79f198.js"/><link as="script" rel="preload" href="/framework-c6410ac4b74f71856d28.js"/><link as="script" rel="preload" href="/app-11ba0897ae6e170019cf.js"/><link as="script" rel="preload" href="/styles-8636a280cbc61d53ad10.js"/><link as="script" rel="preload" href="/ef9668d341e2b6c54db259b86f7e30a3755f1ca1-120d26a5ba92bd694f64.js"/><link as="script" rel="preload" href="/3aa7d83db5eb72830989a908dfb10747247d97cb-593e4006fd0d611c716c.js"/><link as="script" rel="preload" href="/f24a0dfacb1ef885be02157df4ea8093ffd6c393-5c00bc2a1de84a36b201.js"/><link as="script" rel="preload" href="/418ffc66a997cc995fb5d56035c003ac177781e9-8559882675c00cd90fb3.js"/><link as="script" rel="preload" href="/4622c33586ad0108e870f951288ae6aa3beabfab-d386613704ec7a798666.js"/><link as="script" rel="preload" href="/component---src-templates-blog-template-js-d7a0ab3aed8485c52649.js"/><link as="fetch" rel="preload" href="/page-data/blog/k-nearest-neighbors/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><script>
void function() {
  window.__onThemeChange = function() {}

  var preferredTheme
  try {
    preferredTheme = localStorage.getItem('theme')
  } catch (err) { }

  function setTheme(newTheme) {
    if (preferredTheme && document.body.classList.contains(preferredTheme)) {
      document.body.classList.replace(preferredTheme, newTheme)
    } else {
      document.body.classList.add(newTheme)
    }

    window.__theme = newTheme
    preferredTheme = newTheme
    window.__onThemeChange(newTheme)
  }

  window.__setPreferredTheme = function(newTheme) {
    setTheme(newTheme)
    try {
      localStorage.setItem('theme', newTheme)
    } catch (err) {}
  }

  var darkQuery = window.matchMedia('(prefers-color-scheme: dark)')
  darkQuery.addListener(function(e) {
    window.__setPreferredTheme(e.matches ? 'dark' : 'light')
  })

  setTheme(preferredTheme || (darkQuery.matches ? 'dark' : 'light'))
}()
    </script><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div><div class="blogpost-header"><div class="view-page-header"><div class="view-page-header-wrapper"><div class="logo-wrapper"><div class="logo"><div class="logo-img gatsby-image-wrapper" style="position:relative;overflow:hidden"><div aria-hidden="true" style="width:100%;padding-bottom:100%"></div><img aria-hidden="true" src="data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAEEBf/EABYBAQEBAAAAAAAAAAAAAAAAAAIBBP/aAAwDAQACEAMQAAABtmSLU1KeLAa+iHm//8QAHBABAAICAwEAAAAAAAAAAAAAAQACAxIEISNB/9oACAEBAAEFAkuCLNbTNl1lczWHZd9vnHfL/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAECMRD/2gAIAQMBAT8BS5OFaf/EABYRAAMAAAAAAAAAAAAAAAAAAAABEP/aAAgBAgEBPwGo/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAEQESES/9oACAEBAAY/ArMuOU9NLQxx/8QAHBABAAICAwEAAAAAAAAAAAAAAQARIVExYXGR/9oACAEBAAE/IXCVlFftHewg5k+Rq7IcxhphiNu2Lii09n//2gAMAwEAAgADAAAAEKAPfP/EABoRAAICAwAAAAAAAAAAAAAAAAABETEhQfD/2gAIAQMBAT8Qa0OVgt7Ref/EABcRAQEBAQAAAAAAAAAAAAAAAAAxARH/2gAIAQIBAT8Q2uNqH//EAB4QAQACAgEFAAAAAAAAAAAAAAEAESExUUFhcYGh/9oACAEBAAE/EKIFxuEXF2cBIp2+ZREzlLBC1DYAUnrr3irELKYzK1D7GCYEbhQw0tz/2Q==" alt="" style="position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover;object-position:center;opacity:1;transition-delay:500ms"/><noscript><picture><source srcset="/static/7d8b62b999b9a2fb28b8ab360138da37/b13df/photo3.jpg 40w,
/static/7d8b62b999b9a2fb28b8ab360138da37/4e333/photo3.jpg 80w,
/static/7d8b62b999b9a2fb28b8ab360138da37/e75b5/photo3.jpg 160w,
/static/7d8b62b999b9a2fb28b8ab360138da37/40426/photo3.jpg 240w,
/static/7d8b62b999b9a2fb28b8ab360138da37/c01e2/photo3.jpg 320w,
/static/7d8b62b999b9a2fb28b8ab360138da37/b1563/photo3.jpg 3677w" sizes="(max-width: 160px) 100vw, 160px" /><img loading="lazy" sizes="(max-width: 160px) 100vw, 160px" srcset="/static/7d8b62b999b9a2fb28b8ab360138da37/b13df/photo3.jpg 40w,
/static/7d8b62b999b9a2fb28b8ab360138da37/4e333/photo3.jpg 80w,
/static/7d8b62b999b9a2fb28b8ab360138da37/e75b5/photo3.jpg 160w,
/static/7d8b62b999b9a2fb28b8ab360138da37/40426/photo3.jpg 240w,
/static/7d8b62b999b9a2fb28b8ab360138da37/c01e2/photo3.jpg 320w,
/static/7d8b62b999b9a2fb28b8ab360138da37/b1563/photo3.jpg 3677w" src="/static/7d8b62b999b9a2fb28b8ab360138da37/e75b5/photo3.jpg" alt="" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div></div><div class="name"><a href="/blog/" title="back to blog">Roman <br/> Glushko</a></div></div><h2 class="blog-title"><a href="/blog/" title="back to the homepage">Blog</a></h2></div></div><nav class="main-navigation"><ul><li><a rel="home" title="Go Home" href="/">Home</a></li><li><a title="Go to Technical blog" href="/blog/">Blog</a></li><li><a title="Go to Thoughts" href="/thoughts/">Thoughts</a></li><li><a title="Review My CVs" href="/cv/machine-learning-engineer/">CV</a></li></ul></nav></div><main><article class="blog-wrapper"><header><figure class="cover"><div class="cover-filter"><div class="cover-image gatsby-image-wrapper" style="position:relative;overflow:hidden"><div aria-hidden="true" style="width:100%;padding-bottom:75.05882352941177%"></div><img aria-hidden="true" src="data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEDBP/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHPSDlRMP/EABkQAAMBAQEAAAAAAAAAAAAAAAABAhESMv/aAAgBAQABBQJVhVabLFI/PKP/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAaEAACAgMAAAAAAAAAAAAAAAAAARARMTJh/9oACAEBAAY/Asi4bNFluP/EABoQAAMBAQEBAAAAAAAAAAAAAAABESFRMUH/2gAIAQEAAT8hriSE2JGLrIFWuSltSCqvT4Ji2n//2gAMAwEAAgADAAAAEJPP/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8QR//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAECAQE/EFf/xAAeEAEAAgICAwEAAAAAAAAAAAABABExQSFhUZGh0f/aAAgBAQABPxBCYeDInZqMsro8xv1B4AKafq4AURvXXMuBSw5x+y4K3W5//9k=" alt="" style="position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover;object-position:center;opacity:1;transition-delay:500ms"/><noscript><picture><source srcset="/static/dab856139d3de9754d8bc45de82e7772/809fc/pexels-ollie-craig-6398503-min.jpg 850w,
/static/dab856139d3de9754d8bc45de82e7772/4f4f6/pexels-ollie-craig-6398503-min.jpg 1700w,
/static/dab856139d3de9754d8bc45de82e7772/35aef/pexels-ollie-craig-6398503-min.jpg 3400w,
/static/dab856139d3de9754d8bc45de82e7772/9a201/pexels-ollie-craig-6398503-min.jpg 5100w,
/static/dab856139d3de9754d8bc45de82e7772/4c51e/pexels-ollie-craig-6398503-min.jpg 6800w,
/static/dab856139d3de9754d8bc45de82e7772/6b46a/pexels-ollie-craig-6398503-min.jpg 8000w" sizes="(max-width: 3400px) 100vw, 3400px" /><img loading="lazy" sizes="(max-width: 3400px) 100vw, 3400px" srcset="/static/dab856139d3de9754d8bc45de82e7772/809fc/pexels-ollie-craig-6398503-min.jpg 850w,
/static/dab856139d3de9754d8bc45de82e7772/4f4f6/pexels-ollie-craig-6398503-min.jpg 1700w,
/static/dab856139d3de9754d8bc45de82e7772/35aef/pexels-ollie-craig-6398503-min.jpg 3400w,
/static/dab856139d3de9754d8bc45de82e7772/9a201/pexels-ollie-craig-6398503-min.jpg 5100w,
/static/dab856139d3de9754d8bc45de82e7772/4c51e/pexels-ollie-craig-6398503-min.jpg 6800w,
/static/dab856139d3de9754d8bc45de82e7772/6b46a/pexels-ollie-craig-6398503-min.jpg 8000w" src="/static/dab856139d3de9754d8bc45de82e7772/35aef/pexels-ollie-craig-6398503-min.jpg" alt="" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div></div><figcaption class="image-title">Photo by <a href="https://www.pexels.com/@olliecraig1">Ollie Craig</a> from <a href="https://www.pexels.com/photo/roads-with-vehicles-near-buildings-and-autumn-trees-in-city-6398503/">Pexels</a></figcaption></figure><h1>K-Nearest Neighbors üë®‚Äçüë©‚Äçüëß‚Äçüë¶</h1><div class="blog-details"><time class="blog-createdat" dateTime="2021-04-05">Apr 5, 2021</time><span> ‚Ä¢ </span><span class="blog-time2read">5<!-- --> min read</span><div class="theme-switcher"><div class="theme-switcher-toggler"><div class="theme-switcher-track"></div><div class="theme-switcher-thumb"></div><input type="checkbox" class="theme-switcher-input" readonly="" aria-label="Switch between Dark and Light mode"/></div></div></div><ul class="blog-tags"><li>machine learning</li></ul></header><div id="intro" class="blog-divider"></div><div class="content-wrapper"><div class="blog-content-nav-wrapper"><ul class="blog-content-nav"><h2>Content</h2><li class=""><a href="#intro">Intro</a></li></ul></div><div class="content blog-content"><p>K-Nearest Neighbors (a.k.a KNN) is one of the famous machine learning approaches. It's simple, intuitive and useful tool to have in your machine learning toolbox.</p>
<p>KNN is instance-based learning algorithm. It doesn't make any assumption about dataset relations and uses provided samples in order to estimate new items.</p>
<h2 id="intuition" style="position:relative;"><a href="#intuition" aria-label="intuition permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Intuition</h2>
<p>As old saying goes, you are who you surround yourself with. This idea even has mathematical interpretation. In N-dimensional space, points that are closer to each other are more similar, then ones that stand apart. This is the assumption that KNN algorithm makes. It tries to find K nearest known points or samples, also called "neighbors", to some unknown sample. Majority class of neighbors defines class of the unknown sample.</p>
<p><span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; ">
      <span class="gatsby-resp-image-background-image" style="padding-bottom: 74.11764705882352%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACRUlEQVQ4y31Tz08TQRj9OrsgXaAV0ZY1VNNgf+yCkY3giWhiohGJXkEMAWOMSjwINGpJ2S3+iDGaqNEYD548eNaz/gccPHHw4FXReNBEI5qw45vtTNltipN8eW9+vXnffDNEsnHPriP3rBovdhIXSBSjGpYQhySf84lyggMZ+owamxKNHBCax8YC+jvl+AhiL0Xn/998Q1ML2xGDanxDOsZYBaIHgC3AEzzqzmISR3zPuh/wY+lA0WeBs+tq7Y/NK9gGMR3RAz6lhBTGJHZBcEDw91dzsfChp4jaHsOtFMvDqR0xVSeutWXaP8vFybfjvcHGj4yGvxPdFfw30TkInJFCLJyqIXEPChHck+9a/XzZLgj+aT63uL5YDKq5vs80RFH211LWlatfRBpwDNElhN4g2hGjiLIUvAzx85F0jKjzJaLWIaKOUIUvQTAtBDO4M70+4Vqa4kezRoLfHOjmE+jcfqDp7pJzZGZGOauuEDlfiXaAzza9L+7asjD28IZrOc/HTNP37JfcTSWo+irOPO9R3KvUiqJpu6eJkmtIE4LH4XAX8I56xEk4nYfTuOxPwunZgN/q716ds9kWNYuHUtYhmFWF2Y64wUOpi3bQbBMbtEgWz4aCfgpiHTFqFfxzQ5Wp+QO37yFOqvG/Fevht2sF50/FWvhSyl8M1rw7zJJs89upPx+IIjSBvjwAKWcgmOBPB1uejPZ0rlzIOi9Om+nX45neD1f6UmLN2kK+SSHKhToq7oceOniJV+3lxn2rs31NL/UfK03ecKA4ho0AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;"></span>
  <img class="gatsby-resp-image-image" alt="Nearest neighbors defines class of unknown sample" title="Nearest neighbors defines class of unknown sample" src="/static/dbb3359885e09bbbb66596868e221934/c5bb3/k-nearest-neighbors.png" srcset="/static/dbb3359885e09bbbb66596868e221934/04472/k-nearest-neighbors.png 170w,
/static/dbb3359885e09bbbb66596868e221934/9f933/k-nearest-neighbors.png 340w,
/static/dbb3359885e09bbbb66596868e221934/c5bb3/k-nearest-neighbors.png 680w,
/static/dbb3359885e09bbbb66596868e221934/8ae3e/k-nearest-neighbors.png 756w" sizes="(max-width: 680px) 100vw, 680px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;" loading="lazy">
    </span>
          </p>
<div class="image-title">Nearest neighbors defines class of unknown sample</div>
<p>To came from this idea to implementation, we need to define how to find closest points.</p>
<h2 id="closest-points" style="position:relative;"><a href="#closest-points" aria-label="closest points permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Closest Points</h2>
<p>We need to somehow measure distance between samples in order to find closest (hence similar) neighbors. Turned out there are quite a few metrics we can use:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean Distance</a> - straight-line distance between two points</li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Taxicab_geometry">Manhattan (City Block) Distance</a> - distance between two points traversed along one axis at the time at right angle</li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Minkowski_distance">Minkowski Distance</a> - Generalization of Euclidean and Manhattan distances</li>
<li>scikit-learn supports <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric">other metrics</a> as well</li>
</ul>
<p><span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; ">
      <span class="gatsby-resp-image-background-image" style="padding-bottom: 36.47058823529412%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAABN0lEQVQoz01Ry07DMBCc9SNtkzohkBKakKIiEEI9wAEkxKGXiAsgoP0APoAzn8IH8iewaztqpWy8nt0Zz9oAoDQHfxyA1ZJoyQ3jWuqfhTZbl8lWRUwnJByQcHIrq+crBDHgbgzz5tp6aFJEdusuJMU67Y7X6TT3OBKyREby1UhPNq6tEDUIpL00kClLKBd6k8+n/cnzpXdsBJci8MLxnt7WrVuW3j37sCYl5Nf2K+vyfvbQipTViOoJOzqUTb88ax67aiQk4sl4LY+SVxb/HT2d3zT3DWyoQXqmwvm4WtSrmTPhoGgXYoeo5HUMzyAbYC1NhaJA5sFIToocxxw3cFTE/c8EWX83p/nEv8/uMGI3B6E5kEwU9I/39/MNl+y0KBZk5Bb+zrDvXNTnHA3CmLTHqSKnGPB/YCwRrPTRHkoAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;"></span>
  <img class="gatsby-resp-image-image" alt="Different distance metrics and their boundaries" title="Different distance metrics and their boundaries" src="/static/f8f6d672a5fb107dd1d6040218c5088d/c5bb3/distance-metrics.png" srcset="/static/f8f6d672a5fb107dd1d6040218c5088d/04472/distance-metrics.png 170w,
/static/f8f6d672a5fb107dd1d6040218c5088d/9f933/distance-metrics.png 340w,
/static/f8f6d672a5fb107dd1d6040218c5088d/c5bb3/distance-metrics.png 680w,
/static/f8f6d672a5fb107dd1d6040218c5088d/b12f7/distance-metrics.png 1020w,
/static/f8f6d672a5fb107dd1d6040218c5088d/b5a09/distance-metrics.png 1360w,
/static/f8f6d672a5fb107dd1d6040218c5088d/966c1/distance-metrics.png 1694w" sizes="(max-width: 680px) 100vw, 680px" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;" loading="lazy">
    </span>
          </p>
<div class="image-title">Distance metrics and their boundaries in 2D</div>
<p>Euclidean and Manhattan distances are two the most popular distance metrics:</p>
$$
D(x, y)_{euclidean} = \sqrt{\sum_{i=1}^n (x_i-y_i)^2}
$$
$$
D(x, y)_{manhattan} = \sum_{i=1}^n |x_i-y_i|
$$
<p>where $x=(x_1, x_2, ..., x_n)$ and $y=(y_1, y_2, ..., y_n)$ are two points in the N-dimensional space.</p>
<p>Minkowski distance incorporates both Euclidean (when p is 2) and Manhattan (when p is 1) distances and gives additional degree of flexibility via p param:</p>
$$
D(x, y)_{minkowski} = \left(\sum_{i=1}^n (|x_i-y_i|)^p\right)^\frac{1}{p}
$$
<p>These distance metrics are intuitive and fast to compute.</p>
<p>However, all features or dimensions are treated the same way. This may be an issue if features have values in different scales. For example,</p>
<ul>
<li>customer age value is normally between 18 and 100</li>
<li>annual income values lies between 20,000 and 200,000 (and sometimes goes far beyond)</li>
</ul>
<p>When calculating distance metrics, this leads to a situation when major part of similarity actually comes from large scale-value features (like annual income). As a result, the rest of the features will be almost neglected and won't be really taken into account during similarity checking.</p>
<h2 id="standardization" style="position:relative;"><a href="#standardization" aria-label="standardization permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Standardization</h2>
<p>Ignoring small-scale features in the similarity checking is rarely a desired effect. We would like rather to avoid it to get real neighbors of our sample.</p>
<p>We can try to rescale all features to the same range (0-1, for example). This will standardize or normalize features and let them have the same influence in the distance calculations.</p>
$$
Z = \frac{x - \overline{x}}{S}
$$
<p>where $x$ is sample feature value, $\overline{x}$ is a feature mean and $S$ is a feature standard deviation.</p>
<p>Z is a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Standard_score">standard score (or z-score)</a> of the sample feature value. It's a dimensionless quantity and it shows how "unusual" or far sample feature value from the feature mean (which is usual or expected value).</p>
<p>Though normalization changes values of features, it doesn't change feature distribution shape.</p>
<h2 id="dense-dataset" style="position:relative;"><a href="#dense-dataset" aria-label="dense dataset permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Dense Dataset</h2>
<p>We assume that similar samples are close in the N-dimensional feature space. Is this always a case?</p>
<p>Turned out that with growing number of dimensions (or features), number of samples should grow exponentially in order to keep the same density of sample points in the space. High-dimensional space contains far more actual space between points than our intuition may suggest.</p>
<p>This may lead to a situation when distance to neighbors across all feature axis are just slightly bigger than average distance between points. In sparse high-dimensional feature space we are loosing our notion of nearest points.</p>
<p>This is the reason why ratio of features to samples is important for KNN algorithm.</p>
<h2 id="neighborhood-size" style="position:relative;"><a href="#neighborhood-size" aria-label="neighborhood size permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Neighborhood Size</h2>
<p>Effectiveness of applying KNN depends greatly on choosing appropriate size of neighborhood a.k.a <strong>K</strong> hyperparam. In turn this choice is determined by dataset. It's usual to see K in range from 1 to 20 (5 is default size in scikit-learn).</p>
<p>Smaller value of K adds more sensitivity and variance. For noisy datasets this means that given a slightly adjusted dataset we may get a different neighbors.  </p>
<p>Increasing K hyperparam adds more bias to KNN results and smoother decision boundaries. If K param value is too big, KNN will loose its ability to find local structures in the dataset.</p>
<p>In general, neighborhood size is subject of hyperparam tuning. Final decision should be validated on separated validation dataset.</p>
<h2 id="applications" style="position:relative;"><a href="#applications" aria-label="applications permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Applications</h2>
<p>You may find KNN-based classificators and regressors which is a trivial application of KNN idea. Let's review a few less obvious applications where KNN shines even better.</p>
<h3 id="feature-engineering" style="position:relative;"><a href="#feature-engineering" aria-label="feature engineering permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Feature Engineering</h3>
<p>KNN is able to identify local structures in the dataset. This information is useful clue itself and local structure labels may be used in classification/regression pipeline as an additional feature.</p>
<h3 id="neighbors-in-embeddings" style="position:relative;"><a href="#neighbors-in-embeddings" aria-label="neighbors in embeddings permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Neighbors in Embeddings</h3>
<p>KNN algorithm becomes useful when you have embeddings - samples projected into lower dimensional latent space. In embedding space distance means similarity and KNN can suggest similar items. This is especially useful for recommendation systems. In that case, nearest neighbor search can be optimized further by leveraging <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">locality-sensitive hashing</a>, for instance.</p>
<h3 id="anomaly-detection" style="position:relative;"><a href="#anomaly-detection" aria-label="anomaly detection permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Anomaly Detection</h3>
<p>Distances to K nearest neighbors can be viewed as an estimation of local density. If distance is relatively large, this implies that density is low and sample is likely to be an outlier.</p>
<h2 id="summary" style="position:relative;"><a href="#summary" aria-label="summary permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Summary</h2>
<p>I hope we shed a bit of light on theoretical aspects of K-Nearest Neighbor algorithm and proved that this pure idea is still useful after decades of being formulated.</p>
<h2 id="references" style="position:relative;"><a href="#references" aria-label="references permalink" class="anchor before"><svg class="anchor-icon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/">Practical Statistics for Data Scientists by Peter Bruce and Andrew Bruce</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d">k-Nearest Neighbors and the Curse of Dimensionality</a></li>
</ul></div></div><div id="content-end"></div></article><div class="social-share-wrapper"><h3>Share Your Love</h3><button aria-label="Share Via Facebook" title="Share Via Facebook" class="react-share__ShareButton social-share-item facebook" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="facebook" class="svg-inline--fa fa-facebook fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"></path></svg></button><button aria-label="Share Via Twitter" class="react-share__ShareButton social-share-item twitter" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="twitter" class="svg-inline--fa fa-twitter fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></button><button aria-label="Share Via LinkedIn" class="react-share__ShareButton social-share-item linkedin" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="linkedin-in" class="svg-inline--fa fa-linkedin-in fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg></button><button aria-label="Share Via Reddit" class="react-share__ShareButton social-share-item reddit" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="reddit" class="svg-inline--fa fa-reddit fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"></path></svg></button><button aria-label="Add to Pocket" class="react-share__ShareButton social-share-item pocket" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="get-pocket" class="svg-inline--fa fa-get-pocket fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M407.6 64h-367C18.5 64 0 82.5 0 104.6v135.2C0 364.5 99.7 464 224.2 464c124 0 223.8-99.5 223.8-224.2V104.6c0-22.4-17.7-40.6-40.4-40.6zm-162 268.5c-12.4 11.8-31.4 11.1-42.4 0C89.5 223.6 88.3 227.4 88.3 209.3c0-16.9 13.8-30.7 30.7-30.7 17 0 16.1 3.8 105.2 89.3 90.6-86.9 88.6-89.3 105.5-89.3 16.9 0 30.7 13.8 30.7 30.7 0 17.8-2.9 15.7-114.8 123.2z"></path></svg></button></div></main><aside class="blogpost-sidebar"><div class="blog-navigation-wrapper"><h3>Read Other Posts</h3><nav class="blog-navigation"><div class="nav-links"><a rel="prev" class="prev-post" href="/blog/how-i-built-my-ml-workstation/">‚Üê <!-- -->How I built my ML workstation üî¨</a><a class="all-posts" href="/blog/">All Posts</a></div></nav></div></aside><footer><div class="footer-wrapper"><div class="social"><ul class="social-list"><li class="social-item social-linkedin"><a rel="me" itemProp="url" href="https://www.linkedin.com/in/glushko-roman" title="Roman Glushko on LinkedIn" target="blank"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="linkedin-in" class="svg-inline--fa fa-linkedin-in fa-w-14 fa-2x " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg><span>LinkedIn</span></a></li><li class="social-item social-github"><a rel="me" itemProp="url" href="https://github.com/roma-glushko" title="Roman Glushko on Github" target="blank"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github-alt" class="svg-inline--fa fa-github-alt fa-w-15 fa-2x " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path fill="currentColor" d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"></path></svg><span>GitHub</span></a></li><li class="social-item social-email"><a itemProp="email" href="mailto:roman.glushko.m@gmail.com" title="Roman Glushko&#x27;s Email"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" class="svg-inline--fa fa-envelope fa-w-16 fa-2x " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg><span>Email</span></a></li><li class="social-item social-kaggle"><a rel="me" itemProp="url" href="https://www.kaggle.com/glushko" title="Roman Glushko on Kaggle" target="blank"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="kaggle" class="svg-inline--fa fa-kaggle fa-w-10 fa-2x " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M304.2 501.5L158.4 320.3 298.2 185c2.6-2.7 1.7-10.5-5.3-10.5h-69.2c-3.5 0-7 1.8-10.5 5.3L80.9 313.5V7.5q0-7.5-7.5-7.5H21.5Q14 0 14 7.5v497q0 7.5 7.5 7.5h51.9q7.5 0 7.5-7.5v-109l30.8-29.3 110.5 140.6c3 3.5 6.5 5.3 10.5 5.3h66.9q5.25 0 6-3z"></path></svg><span>Kaggle</span></a></li><li class="social-item social-twitter"><a rel="me" itemProp="url" href="https://twitter.com/roma_glushko" title="Roman Glushko on Twitter" target="blank"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="twitter" class="svg-inline--fa fa-twitter fa-w-16 fa-2x " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg><span>Twitter</span></a></li><li class="social-item social-patreon"><a rel="me" itemProp="url" href="https://www.patreon.com/roma_glushko" title="Support my content on Patreon" target="blank"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="patreon" class="svg-inline--fa fa-patreon fa-w-16 fa-2x " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M512 194.8c0 101.3-82.4 183.8-183.8 183.8-101.7 0-184.4-82.4-184.4-183.8 0-101.6 82.7-184.3 184.4-184.3C429.6 10.5 512 93.2 512 194.8zM0 501.5h90v-491H0v491z"></path></svg><span>Patreon</span></a></li></ul></div><div class="copyright">Roman Glushko ¬© 1996 - <!-- -->2021<!-- --> <br/><a rel="license" href="https://creativecommons.org/licenses/by/4.0/" title="Content is published under CC BY 4.0 license">CC BY 4.0</a></div></div></footer><span></span><div class="mathjax"></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script>
  
  function gaOptout(){document.cookie=disableStr+'=true; expires=Thu, 31 Dec 2099 23:59:59 UTC;path=/',window[disableStr]=!0}var gaProperty='UA-148139633-1',disableStr='ga-disable-'+gaProperty;document.cookie.indexOf(disableStr+'=true')>-1&&(window[disableStr]=!0);
  if(true) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  }
  if (typeof ga === "function") {
    ga('create', 'UA-148139633-1', 'auto', {"alwaysSendReferrer":true});
      ga('set', 'anonymizeIp', true);
      
      
      
      
      }</script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/blog/k-nearest-neighbors/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"app":["/app-11ba0897ae6e170019cf.js"],"component---cache-caches-gatsby-plugin-offline-app-shell-js":["/component---cache-caches-gatsby-plugin-offline-app-shell-js-f94ad01b5542d484ff45.js"],"component---src-pages-404-js":["/component---src-pages-404-js-d082b1c6963c413420f1.js"],"component---src-pages-blog-js":["/component---src-pages-blog-js-fab8373c7d697f156720.js"],"component---src-pages-cv-ecommerce-developer-js":["/component---src-pages-cv-ecommerce-developer-js-0935bcbe3afc0bbb2274.js"],"component---src-pages-cv-machine-learning-engineer-js":["/component---src-pages-cv-machine-learning-engineer-js-f24d6f01b4c07c5fcd37.js"],"component---src-pages-index-js":["/component---src-pages-index-js-e9e6323136d633413e1e.js"],"component---src-pages-indexv-2-js":["/component---src-pages-indexv-2-js-2209599bd83bbd273e14.js"],"component---src-pages-lab-js":["/component---src-pages-lab-js-4397f5d912ef6a151438.js"],"component---src-pages-lab-rock-paper-scissors-index-js":["/component---src-pages-lab-rock-paper-scissors-index-js-04ffc68c0edce4fc3bdf.js"],"component---src-pages-nn-design-js":["/component---src-pages-nn-design-js-3d043d767e138ced8144.js"],"component---src-pages-thoughts-js":["/component---src-pages-thoughts-js-5eb574f1ebe2e9dcbdbc.js"],"component---src-templates-blog-template-js":["/component---src-templates-blog-template-js-d7a0ab3aed8485c52649.js"],"component---src-templates-thought-template-js":["/component---src-templates-thought-template-js-7e5169519887d3bc8237.js"]};/*]]>*/</script><script src="/component---src-templates-blog-template-js-d7a0ab3aed8485c52649.js" async=""></script><script src="/4622c33586ad0108e870f951288ae6aa3beabfab-d386613704ec7a798666.js" async=""></script><script src="/418ffc66a997cc995fb5d56035c003ac177781e9-8559882675c00cd90fb3.js" async=""></script><script src="/f24a0dfacb1ef885be02157df4ea8093ffd6c393-5c00bc2a1de84a36b201.js" async=""></script><script src="/3aa7d83db5eb72830989a908dfb10747247d97cb-593e4006fd0d611c716c.js" async=""></script><script src="/ef9668d341e2b6c54db259b86f7e30a3755f1ca1-120d26a5ba92bd694f64.js" async=""></script><script src="/styles-8636a280cbc61d53ad10.js" async=""></script><script src="/app-11ba0897ae6e170019cf.js" async=""></script><script src="/framework-c6410ac4b74f71856d28.js" async=""></script><script src="/webpack-runtime-b8401f474d461e79f198.js" async=""></script></body></html>