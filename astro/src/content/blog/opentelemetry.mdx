---
id: 62b87a6b15e35cd77d0d0ddd
title: "OpenTelemetry"
draft: false
excerpt: "How OTEL SDK and Collector work"
pubDate: "Nov 7, 2023"
keywords:
  - open source
  - architecture
  - observability
cover:
    image: /blog/opentelemetry/jennifer-lim-tamkican-Utt2Kc2JyAc-unsplash.jpg
    credits: Photo By <a target="_blank" rel="noopener noreferrer nofollow" href="https://unsplash.com/photos/black-and-white-telescope-with-tripod-Utt2Kc2JyAc">Jennifer Lim-Tamkican</a>
---

import Note from '@components/content/Note.tsx'

With the raise of the open source community, people and organizations don't really want to invest into proprietary protocols and standards anymore.
Instead, it's a mainstream to pick a widely-recognized open source project as a basis to build on top of it.

In the observability domain, there are enough open source protocols that cover some of the thee key pilars:

- `Logs` - stdout and stderr streams your services or application outputs while running in the container.
- `Metrics` - aggregations of some measurements over a period of time
- `Traces` - a visualization of the steps or the execution path that your workflow passed

![The Three Pillars of Observability](/blog/opentelemetry/observability-pillars.png "The Three Pillars of Observability")
<div class="image-title">The Three Pillars of Observability</div>

When you think about metrics, [Prometheus](https://prometheus.io/) and [statsd](https://github.com/statsd/statsd) may come to your mind. 
On the traces side, there are [Jaeger](https://www.jaegertracing.io/) and [Zipkin](https://zipkin.io/).
There are also various beats that could scrap your logs like [fluentd](https://www.fluentd.org/) or [Grafana Loki](https://grafana.com/oss/loki/).

Now if you want to cover the all tree pilars, you need to pick a combination of collectors/protocols to cover each one (well, some are capable of covering a few pilars).
However, there are still a few problems left:

- even though the protocols are open source, they may still tightly connect your application to the underlying collector or storage, so it won't be that easy to switch gears and use something else
- we have divided observability into three pieces, but in reality they are three different signals or point of views on the application work, so we may get the whole picture and max value out of them when they are well connected and correlated for us

![The observability singals are unconnected](/blog/opentelemetry/observability-signals-unconnected.png "The observability singals are unconnected")
<div class="image-title">The observability singals are unconnected when you use separate highly specifalized protocols</div>

To sum it up, we want to have:

- one open source vendor-lock-free protocol to rule all observability signals
- a stable abstruction for us for the underlying observability storage(s) without a need to migrate our service every time that storage changes
- a coverage of popular programming languages, not to be limited in what our tech stack looks like

[TODO: Add a diagram where all siganls are connected]

This idea is so compeling that at some point there were two projects, [OpenTracing](https://opentracing.io/) and [OpenCensus](https://opencensus.io/), that were trying to fill the gap. 
They were trying to compete with each other, but there was not the right context to do that as they were working on the big unbiquitius protocol idea, so it didn't make sense to try to conquer some market shares, but rather it made to consolidate their effort and came there much quicker.
And that's what happened. Both projects were merged into one known as [OpenTelemetry](https://opentelemetry.io/) (aka OTEL). 

OpenTelemetry is an observability framework and an active [CNCF project](https://www.cncf.io/projects/opentelemetry/) that provides a vendor-neutral and tool-agnostic way to collect observability signals across your heterogeneus system.

[TODO: Don't like that the first part of the intro is lil bit unconnected with this description]

In this blog post, we will review:

- how OpenTelemetry instrumentation works on the application side taking [Python's SDK](https://github.com/open-telemetry/opentelemetry-python/) as an example
- the architecture of OpenTelemetry Collector and its capabilities

## SDK

Collecting logs, metrics and traces in a unified way across services implemented in different technical stacks is the central task of OpenTelemetry. 
To get there, OpenTelemetry provides:

- SDKs for [11+ the most popular languages](https://opentelemetry.io/docs/instrumentation/) (like Python, Go, NodeJS, Rust, Java, etc) that inits OTEL core components
- library-specific instrumentations that provides tool/framework-specific signals and context automagically (e.g. [Starlette](https://www.starlette.io/), [HTTPX](https://www.python-httpx.org/), [aiopika](https://aio-pika.readthedocs.io/en/latest/) instrumentations, so on)

The third thing you could do is to further instrument your codebase with business logic specific traces and metrics.

This process is generally known as codebase *instrumentation*.

There are two ways to setup OTEL in your application:

- automatic - when you run some agent before the main application entry point that configures OpenTelemetry (but not all languages support it, for example, Golang doesn't)
- manual - when you configure OpenTelemetry yourself to start collecting your observability signals.

To understand how OpenTelemetry SDK is designed and implemented, we will delve into the manual setup.

This is what it takes you to manually setup Python's service:

```python {11-12,21-22,24,27}
from opentelemetry import metrics, trace
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import (
    ConsoleMetricExporter,
    PeriodicExportingMetricReader,
)
from opentelemetry.sdk.resources import SERVICE_NAME, SERVICE_VERSION, Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter

span_processor = BatchSpanProcessor(ConsoleSpanExporter())
metric_reader = PeriodicExportingMetricReader(ConsoleMetricExporter())

resource = Resource(
    attributes={
        SERVICE_NAME: "notifications",
        SERVICE_VERSION: "v42",
    }
)

trace_provider = TracerProvider(resource=resource)
metrics_provider = MeterProvider(metric_readers=[metric_reader])

trace_provider.add_span_processor(span_processor)

# Sets the global default providers
trace.set_tracer_provider(trace_provider)
metrics.set_meter_provider(metrics_provider)

# Creates a custom tracer from the global provider
tracer = trace.get_tracer("notifications")

# Creates a custom meter from the global provider
meter = metrics.get_meter("notifications")
```

TODO: Add usage of tracer and meter

Let's try to unpack what's going on there.

## Resources

First of all, let's pay attention to the `Resource{:.entity.name}`. It is an abstruction around entities that could generate signals:

```python
from opentelemetry.sdk.resources import SERVICE_NAME, SERVICE_VERSION, Resource

resource = Resource(
    attributes={
        SERVICE_NAME: "notifications",
        SERVICE_VERSION: "v42",
    }
)
```

Right off the bat, `Resource{:.entity.name}` illustrates a few important concepts in the observability domain.

Most context in the observability signals are going to be conveyed via `attributes{:.variable.parameter}` or tags which are essentially a key-value pairs. Then, observability backend could do some processing of these values to correplate various pieces of information.

OTEL strives to standardize the attribute names to keep them consistent across your system. That's why they come as constants. 
In practice, this is an extremely dounting and challenging task, especially when multiple teams are working on different services in parallel.

## Providers

Python's SDK comes with two modules called `trace{:python}` and `metrics{:python}`. 
Both modules contain a global variable that holds the current provider and a setter method like `set_tracer_provider(){:.entity.name.function}` to configure it.

If you don't want to configure a real provider, there are also `NoOpTracerProvider{:.entity.name.class}` or `NoOpMeterProvider{:.entity.name.class}` that are helpful to keep all custom instrumentaions in place without a need for removing them via `if{:python}`s, for example (this is [the null object pattern](https://en.wikipedia.org/wiki/Null_object_pattern) in action).

<Note title={`Design Patterns in Wild`} type={`info`}>
All providers have NoOp implementations which is a great example of [the null object pattern](https://refactoring.guru/introduce-null-object). Since telemetry could spread across all codebase, it would be disastrous to have `if{:python}` statements everywhere we had it in case we have no proper observability setup under some circumstances like like during automated testing.
</Note>


Then, the rest of codebase refers the global providers when it needs to create traces or metrics.

![The Tracer & Meter Registries](/blog/opentelemetry/opentelemetry-registries.png "The Tracer & Meter Registries")
<div class="image-title">The Tracer & Meter Registries</div>

## Traces

The `TracerProvider{:.entity.name.class}` is basically a factory that creates `Tracer{:.entity.name.class}`s and passes most of its params down to a `Tracer{:.entity.name}`.
`Tracer{:.entity.name}`s represent the specific `trace` that could contain many `spans`.

Conceptually, `traces` are connected to the specific workflow or operation in the system. 
That's why their names should the same for the same processes e.g. `GET /users/{user_id}/` may represent all requests to an API that returns user's data. 

Then, `spans` may represent some steps in your workflow. For instance, to get the user information, you may need to perform a request to your database. 
That action may be wrapped into a `span`. Additional `attributes{:.variable.parameter}` could be added to the span to record some events, action result statuses, etc.
In the end, all `spans` forms a hierarical tree that could be viewed in the observability backend.

[TODO: add an example of trace view]

### Spans

In OTEL protocol, `traces` are rather virtual entities that hold some execution context. The real data points that are being collected, processed and exported are `spans`.

The `Span{:.entity.name.class}` consists of:

- name - the human-friendly title of the step
- kind like internal, server, client, producer, consumer
- status like `not set`, `ok`, `error`
- the span context
- the parent span context
- the resource context
- the trace or instrumentation scope
- span `attributes{:.variable.parameter}`
- events - special entities with the name, timestamp and own set of `attributes{:.variable.parameter}`
- start_time & end_time as `time.time_ns(){:python}`

The `Span{:.entity.name.class}` class is also a [context manager](https://book.pythontips.com/en/latest/context_managers.html#context-managers), so when it starts and exits, the span signals the `SpanProcessor` about that.

### Span Sampling

There is also an optional opportunity to configure a span sampler.

Sampling is a way to filter out some spans or other data points if they match some specific criteria or just randomly.
The major reasons to sample are:

- to optimize the cost of ingesting and storing observability signals
- filter out boring regular data and mostly keep interesting one e.g. spans with errors, that took more than threshold
- merely filter based on presense or abcense of `attributes{:.variable.parameter}` 

Sampling on the SDK side could allow filtering as soon as possible in the pipeline (or the head sampling).

OTEL comes with a few span samplers out of the box:

- `StaticSampler{:.entity.name.class}` that always or never drops spans
- `TraceIdRatio{:.entity.name.class}` that probabilistically drops a given portion of spans

![OTEL SDK Span Pipeline](/blog/opentelemetry/otel-traces-pipeline.png "OTEL SDK Span Pipeline")
<div class="image-title">OTEL SDK Span Pipeline</div>

These two samplers could be also configured to respect parent span decisions, so if the parent span was dropped, all its children spans would be eliminated, too.

<Note title={`Design Patterns in Wild`} type={`info`}>
    Parent span aware sampling is implemented using [the composite design pattern](https://refactoring.guru/design-patterns/composite).
    
    The parent-based sampler wraps the static or ration-based sampler mentioned above and adds a logic to propagate parent span's sampling decision.
</Note>

### Span Processors

When spans end, they come to span processors. Span processing is the last stage of span's lifecycle before it's exported outside of the service. OpenTelemetry uses it to batch spans and multiplex them to multiple exporters.

The `BatchSpanProcessor` collects spans and exports them on schedule or when its queue is full. For that, it maintains a separate daemon thread where this logic is executed.

There are also two `MultiSpanProcessors` that operates on a list of span processors and dispatch them sequentually or concurrently.

<Note title={`Design Patterns in Wild`} type={`info`}>
    Guess what? MultiSpanProcessors are yet another [composites](https://refactoring.guru/design-patterns/composite).
</Note>

The concurrent span processing happens via [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor).

## Metrics

Metrics are another observability pillar we are going to review next. The mechanism of collecting metrics is a bit different to traces. 

In case of traces, user code actively creates trace spans and as soon as they are completed, OpenTelemetry could process and export them.
The metrics dynamic is much more continues, so the collection really ends when the service shuts down. 
Since service uptime could be measured in days if not weeks, we need to take a different approach here to export all measurements in between like doing metric data aggregations periodically over time window.

Just like in the case of traces, OpenTelemetry provides <span class="orange">MeterProvider</span> that bridge all metrics with metric exporters. 
<span class="orange">MeterProvider</span> creates new instances of `Meter{:.entity.name.class}`. 
Meters are instrumentation-specific measurement components. Each OTEL instrumentation library creates an own meter (e.g. HTTP client or server meters).
When you do your custom measurements it's alright to have one global meter per service (but you certainly could have more).

### Metric Instruments

Now, having a meter, you could create specific metrics (a.k.a. metric instruments) that you want to measure or observe. Generally, OTEL divides metrics into two categories:

- `Synchronious metrics` - theres are metrics that you measure directly right in your service workflows, so you observe them as soon as the event happens (e.g. a service increases a counter metric in the user login workflow)
- `Asynchronius (or observable) metrics` - these metrics are read from "external" sources, so you just observe an aggregated or in-time statistics instead of measure the value directly (e.g. number of items in a queue given that you cannot instrument the queue directly and you could just read its size property)

OpenTelemetry supports the following metric types:

- <span class="green">Counter</span> (and <span class="green">Observable Counter</span>) - an ever-growing (or monotonically increasing) value (for example, the number of requests processed by service)
- <span class="green">UpDownCounter</span> (and <span class="green">Observable UpDownCounter</span>) - a value that could grow or fall (for example, the number of in-flight requests)
- <span class="green">Histogram</span> (and <span class="green">Observable Histogram</span>) - suitable for measurements on which you want to calculate statistics (for example, request latency)
- <span class="green">Gauge</span> - just like the observable <span class="green">UpDownCounter</span>, but each measurement is treated as a separate data point, so they are not summed up (for example, CPU or RAM utilizations)

### Views & Aggregations

With our metrics defined, we could start measuring, aggregating and collecting actual values.

Metric instruments don't collect data directly but rather sending it to `MeasurementConsumer{:.entity.name.class}` that is a global component inited on the <span class="orange">MeterProvider's</span> level.
`MeasurementConsumer{:.entity.name.class}` collects data for each and all `MetricReader{:.entity.name.class}`s configured on the provider.

![The Architecture of OTEL Metrics](/blog/opentelemetry/otel-metrics-architecture.png "The Architecture of OTEL Metrics")
<div class="image-title">The Architecture of OTEL Metrics</div>

Thinking about our source metrics data, it's just arrays of numbers with attributes (or one number at the time in case of observable instruments), so there are possible multiple options how they could be aggregated.
OpenTelemetry provides a great flexibility there.

![OTEL Metrics, Views, Aggregations](/blog/opentelemetry/otel-metrics-logical-structure.png "OTEL Metrics, Views, Aggregations")
<div class="image-title">OTEL Metrics, Views, Aggregations</div>

First of all, aggregations could be configured on the metric exporter side. Maybe, you are an observability backend vendor like DataDog or Chronosphere and you come up with a better or specific way to deal with metric data points. This would be an opportunity for you to adjust exported data.

Then, you could leverage a concept of <span class="yellow">views</span> to override the config further. <span class="yellow">Views</span> effectively allow to specify the aggregation strategy per metric insturment and its metadata (e.g. name, attributes).

By default, OTEL implements the following aggregations:

- <span class="yellow">Drop Aggregation</span> - a way to drop metric collection completely.
- <span class="yellow">Last Value Aggregation</span> - keeps the last aggregated value until it's collected (used by gauges).
- <span class="yellow">Sum Aggregation</span> - arithmetic sum of provided data points (used by counters).
- <span class="yellow">Explicit Bucket Histogram Aggregation</span> - assigns collected data points to one of 15 predefined buckets. Besides that, it collects sum, count, min and max across given data.
- <span class="yellow">Exponential Bucket Histogram Aggregation</span> - similar to the explicit bucket aggregation, but buckets are generated dynamically exponentially growing size of the next bucket and much more fine grained (by default, there are 150 buckets).

The sum and histogram aggregations may collect data between probes as:

- <span class="yellow">deltas</span> e.g. differences between the previous aggregated stats (e.g. sums, counts) and the current ones.
- <span class="yellow">cumulative data</span> e.g. the previous and the current stats are summed up (so the values keep increasing over time).

This is called <span class="yellow">aggregation temporality</span>.

### Metric Readers

Metrics are read on schedule by `MetricReader{:.entity.name.class}` like `PeriodicExportingMetricReader{:.entity.name.class}` that 


## Logs

## Context Propagation

## Signal Export

## References

- [[OpenTelemetry] Python SDK](https://opentelemetry.io/docs/instrumentation/python/)
- [[Github] OTEL Collector](https://github.com/open-telemetry/opentelemetry-collector)
- [[OpenTelemetry] OTEL Collector](https://opentelemetry.io/docs/collector/)
- [[OpenTelemetry] OpenTelemetry Collector Architecture](https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/design.md)
- [[OpenTelemetry] Sampling](https://opentelemetry.io/docs/concepts/sampling/)